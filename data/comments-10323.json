[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1018981493",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10323#issuecomment-1018981493",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10323",
        "id": 1018981493,
        "node_id": "IC_kwDOCVq1mM48vGx1",
        "user": {
            "login": "edgchen1",
            "id": 18449977,
            "node_id": "MDQ6VXNlcjE4NDQ5OTc3",
            "avatar_url": "https://avatars.githubusercontent.com/u/18449977?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/edgchen1",
            "html_url": "https://github.com/edgchen1",
            "followers_url": "https://api.github.com/users/edgchen1/followers",
            "following_url": "https://api.github.com/users/edgchen1/following{/other_user}",
            "gists_url": "https://api.github.com/users/edgchen1/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/edgchen1/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/edgchen1/subscriptions",
            "organizations_url": "https://api.github.com/users/edgchen1/orgs",
            "repos_url": "https://api.github.com/users/edgchen1/repos",
            "events_url": "https://api.github.com/users/edgchen1/events{/privacy}",
            "received_events_url": "https://api.github.com/users/edgchen1/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-22T00:04:19Z",
        "updated_at": "2022-01-22T00:04:19Z",
        "author_association": "MEMBER",
        "body": "Can you please also provide a model?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1018981493/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1021590150",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10323#issuecomment-1021590150",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10323",
        "id": 1021590150,
        "node_id": "IC_kwDOCVq1mM485DqG",
        "user": {
            "login": "edgchen1",
            "id": 18449977,
            "node_id": "MDQ6VXNlcjE4NDQ5OTc3",
            "avatar_url": "https://avatars.githubusercontent.com/u/18449977?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/edgchen1",
            "html_url": "https://github.com/edgchen1",
            "followers_url": "https://api.github.com/users/edgchen1/followers",
            "following_url": "https://api.github.com/users/edgchen1/following{/other_user}",
            "gists_url": "https://api.github.com/users/edgchen1/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/edgchen1/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/edgchen1/subscriptions",
            "organizations_url": "https://api.github.com/users/edgchen1/orgs",
            "repos_url": "https://api.github.com/users/edgchen1/repos",
            "events_url": "https://api.github.com/users/edgchen1/events{/privacy}",
            "received_events_url": "https://api.github.com/users/edgchen1/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-25T20:41:49Z",
        "updated_at": "2022-01-25T20:41:49Z",
        "author_association": "MEMBER",
        "body": "When creating the ORT tensor, you are telling ORT that the supplied buffer is in GPU memory when it is actually in CPU memory.\r\n```c++\r\nOrt::MemoryInfo info(\"Cuda\", OrtDeviceAllocator, 0, OrtMemTypeDefault);  // GPU memory\r\n// ...\r\nstd::vector<float> input_tensor_video_values(video_tensor_size);  // CPU memory\r\nfor (size_t i = 0; i < video_tensor_size; ++i)\r\n{\r\n    input_tensor_video_values[i] = 1;\r\n}\r\n// ...\r\nOrt::Value input_tensor_video = Ort::Value::CreateTensor<float>(\r\n  info /* specifying GPU memory */,\r\n  input_tensor_video_values.data() /* passing CPU memory pointer */,\r\n  ...);\r\n```\r\n\r\nSo something like an illegal memory access is expected.\r\n\r\nYou can tell ORT that the value is on CPU and it will be copied over to the GPU as needed:\r\nhttps://github.com/microsoft/onnxruntime-inference-examples/blob/ba19aa8f14366aaf8d43374320a897bf98f695f8/c_cxx/MNIST/MNIST.cpp#L34-L35\r\n\r\nAlternatively, if the input copy incurs too much performance overhead, you can set up some GPU memory appropriately (e.g., with cudaMalloc and cudaMemcpy) and pass that to CreateTensor() instead.\r\n\r\nAdmittedly, this is not clear from the documentation for CreateTensor.\r\nCreateSparseTensor has more info: https://onnxruntime.ai/docs/api/c/struct_ort_1_1_value.html#a07bd3c4da603a8fc23ea6e1f86482f6b",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1021590150/reactions",
            "total_count": 2,
            "+1": 2,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]