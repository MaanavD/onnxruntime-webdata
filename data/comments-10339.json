[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1017859075",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10339#issuecomment-1017859075",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10339",
        "id": 1017859075,
        "node_id": "IC_kwDOCVq1mM48q0wD",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-20T19:42:44Z",
        "updated_at": "2022-01-20T19:48:39Z",
        "author_association": "MEMBER",
        "body": "It is likely that the graph is slightly different (caused by different version of PyTorch or transformers, or your modifications), that caused Attention not fused. Please refer to our developer guide:\r\nhttps://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/transformers/Dev_Guide.md\r\n\r\n@dave-rtzr, you can debug a little into the following fusion to see which part of subgraph is not matched and modify it accordingly: https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/transformers/fusion_gpt_attention_no_past.py\r\nYou can use Visual Studio code, and launch `python optimizer.py --input ./model.onnx --output ./out.onnx --model_type gpt2 --num_heads 12 --hidden_size 768 --input_int32 --float16 --use_gpu --opt_level 0` in that source directory to debug it.\r\n\r\nFeel free to contribute some improvements to make the fusion more robust.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1017859075/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]