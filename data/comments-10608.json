[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1047290850",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10608#issuecomment-1047290850",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10608",
        "id": 1047290850,
        "node_id": "IC_kwDOCVq1mM4-bGPi",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-02-21T23:16:10Z",
        "updated_at": "2022-02-21T23:16:10Z",
        "author_association": "MEMBER",
        "body": "That seems excessively large. Can you separate out things so you're just timing the call to `run` (i.e. create the input prior to that call)? I'd also suggest using `time.perf_counter()` instead of `time.time()`.\r\n\r\nOn the first call to `run` we setup a number of structures to make future calls faster, however I have never seen the speed increase by 5000x (even a 2x difference would be exceptional) so I think something else is causing the delay. \r\n\r\nAre your results repeatable? If so could you please share the model? ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1047290850/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1047473949",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10608#issuecomment-1047473949",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10608",
        "id": 1047473949,
        "node_id": "IC_kwDOCVq1mM4-by8d",
        "user": {
            "login": "Z-XQ",
            "id": 37925249,
            "node_id": "MDQ6VXNlcjM3OTI1MjQ5",
            "avatar_url": "https://avatars.githubusercontent.com/u/37925249?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Z-XQ",
            "html_url": "https://github.com/Z-XQ",
            "followers_url": "https://api.github.com/users/Z-XQ/followers",
            "following_url": "https://api.github.com/users/Z-XQ/following{/other_user}",
            "gists_url": "https://api.github.com/users/Z-XQ/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Z-XQ/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Z-XQ/subscriptions",
            "organizations_url": "https://api.github.com/users/Z-XQ/orgs",
            "repos_url": "https://api.github.com/users/Z-XQ/repos",
            "events_url": "https://api.github.com/users/Z-XQ/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Z-XQ/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-02-22T06:42:30Z",
        "updated_at": "2022-02-22T06:42:30Z",
        "author_association": "NONE",
        "body": "Thank you for your suggestions!  And yes, the results are repeatable. I have uploaded the model to Github: [https://github.com/Z-XQ/onnx_deploy_python.git](url)\r\n\r\n**The new code:** \r\n`\r\nimport time\r\nimport onnx\r\nimport numpy as np\r\nimport onnxruntime as ort\r\n\r\n\r\nstart = time.time()\r\nprint(\"load model ....\")\r\nmodel = onnx.load_model(r'D:\\code\\python\\PycharmProjects\\onnx_deploy_python\\qr_seg_df_200_0217.onnx')\r\nprint(\"load model success!!!: {}\".format(time.time()-start))\r\n\r\nstart = time.time()\r\nprint(\"open InferenceSession ....\")\r\nort_session = ort.InferenceSession(r'D:\\code\\python\\PycharmProjects\\onnx_deploy_python\\qr_seg_df_200_0217.onnx')\r\nprint(\"open InferenceSession success!!! time: {}\".format(time.time() - start))\r\n\r\nwhile 1:\r\n    start = time.time()\r\n    outputs = ort_session.run(None, {'input.1': np.random.randn(1, 3, 512, 512).astype(np.float32)})  # inputs is same to orig\r\n    print(\"run one image time: {}\".format(time.time() - start))\r\n`\r\n\r\n**The print log info**:\r\nload model ....\r\nload model success!!!: 0.08663201332092285\r\nopen InferenceSession ....\r\nopen InferenceSession success!!! time: **368.0281958580017**\r\nrun one image time: **207.29865789413452**\r\nrun one image time: 0.04680299758911133\r\nrun one image time: 0.037839412689208984\r\nrun one image time: 0.037839412689208984\r\nrun one image time: 0.03883624076843262\r\nrun one image time: 0.03982734680175781\r\n\r\nIf I run this model on another computer (this computer converts *.pth model into the onnx model), I won't have this problem. \r\nThe only difference of system information: original computer gpu version of generating onnx model is gtx 2080, and  gtx 3090  is the computer with the problem.\r\n\r\n**The print log info**:\r\nload model ....\r\nload model success!!!: 0.04687833786010742\r\nopen InferenceSession ....\r\nopen InferenceSession success!!! time: **0.6851935386657715**\r\nrun one image time: **0.5457143783569336**\r\nrun one image time: 0.031937599182128906\r\nrun one image time: 0.026904821395874023\r\nrun one image time: 0.024933338165283203\r\nrun one image time: 0.02496170997619629\r\nrun one image time: 0.025929927825927734\r\nrun one image time: 0.024905920028686523\r\n\r\nI hope I have described the problem clearly and look forward to your reply. Thank you!\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1047473949/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1047554269",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10608#issuecomment-1047554269",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10608",
        "id": 1047554269,
        "node_id": "IC_kwDOCVq1mM4-cGjd",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-02-22T08:44:47Z",
        "updated_at": "2022-02-22T08:44:47Z",
        "author_association": "MEMBER",
        "body": "Slightly modified script to just time the calls and not do an unnecessary load with onnx:\r\n\r\n```python\r\nimport onnxruntime as ort\r\nimport time\r\nimport numpy as np\r\n\r\nprint(\"open InferenceSession ....\")\r\nstart = time.time()\r\nort_session = ort.InferenceSession('qr_seg_df_200_0217.onnx')\r\nend = time.time()\r\nprint(\"open InferenceSession time: {}\".format(end - start))\r\n\r\ninput = np.random.randn(1, 3, 512, 512).astype(np.float32)\r\nfor i in range(0, 10):\r\n    start = time.time()\r\n    outputs = ort_session.run(None, {'input.1': input})\r\n    end = time.time()\r\n    print(\"run one image time: {}\".format(end - start))\r\n```\r\n\r\nOutput is roughly as I'd expect. First call takes longer but not 5000x longer. What version of .\r\n\r\n```\r\nopen InferenceSession ....\r\nopen InferenceSession time: 0.22037076950073242\r\nrun one image time: 0.132584810256958\r\nrun one image time: 0.12442970275878906\r\nrun one image time: 0.10365462303161621\r\nrun one image time: 0.11006546020507812\r\nrun one image time: 0.10300135612487793\r\nrun one image time: 0.10101985931396484\r\nrun one image time: 0.10205388069152832\r\nrun one image time: 0.10206198692321777\r\nrun one image time: 0.10125160217285156\r\nrun one image time: 0.10006308555603027\r\n```\r\n\r\nWindows 11. ORT version 1.10.0 installed using pip.\r\n\r\nI also tried with ORT version 1.7.0 (can't test 1.6.0 as I have a python version it doesn't support). Performance is not as good, but the difference between the first and subsequent calls to `run` is similar. Definitely not taking 200 seconds. \r\n\r\n```\r\nopen InferenceSession ....\r\nopen InferenceSession time: 0.2311258316040039\r\nrun one image time: 0.31304025650024414\r\nrun one image time: 0.1999988555908203\r\nrun one image time: 0.14400315284729004\r\nrun one image time: 0.1419987678527832\r\nrun one image time: 0.12608051300048828\r\nrun one image time: 0.14208745956420898\r\nrun one image time: 0.13152694702148438\r\nrun one image time: 0.14204096794128418\r\nrun one image time: 0.1420271396636963\r\nrun one image time: 0.12600159645080566\r\n```\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1047554269/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1047576661",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10608#issuecomment-1047576661",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10608",
        "id": 1047576661,
        "node_id": "IC_kwDOCVq1mM4-cMBV",
        "user": {
            "login": "Z-XQ",
            "id": 37925249,
            "node_id": "MDQ6VXNlcjM3OTI1MjQ5",
            "avatar_url": "https://avatars.githubusercontent.com/u/37925249?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Z-XQ",
            "html_url": "https://github.com/Z-XQ",
            "followers_url": "https://api.github.com/users/Z-XQ/followers",
            "following_url": "https://api.github.com/users/Z-XQ/following{/other_user}",
            "gists_url": "https://api.github.com/users/Z-XQ/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Z-XQ/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Z-XQ/subscriptions",
            "organizations_url": "https://api.github.com/users/Z-XQ/orgs",
            "repos_url": "https://api.github.com/users/Z-XQ/repos",
            "events_url": "https://api.github.com/users/Z-XQ/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Z-XQ/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-02-22T09:10:52Z",
        "updated_at": "2022-02-22T09:54:31Z",
        "author_association": "NONE",
        "body": "Thank you for your comments and suggestions. I run the same code as your suggestions, but the problem still exits.  maybe I should try to test the model with ORT version 1.7.0. \r\n\r\nThank you!\r\n\r\n**System information**\r\nwindow10:\r\nonnxruntime1.6.0:\r\nPython version 3.7\r\nVisual Studio version 2015\r\nCUDA/cuDNN version: cuda10.2, cudnn8.0.3\r\nGPU model and memory: gtx 3090\r\n\r\n\r\n\r\n![1645520432(1)](https://user-images.githubusercontent.com/37925249/155098052-555302a3-b36f-4200-b5a7-716b07afffcc.png)\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1047576661/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1047971456",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10608#issuecomment-1047971456",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10608",
        "id": 1047971456,
        "node_id": "IC_kwDOCVq1mM4-dsaA",
        "user": {
            "login": "snnn",
            "id": 856316,
            "node_id": "MDQ6VXNlcjg1NjMxNg==",
            "avatar_url": "https://avatars.githubusercontent.com/u/856316?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/snnn",
            "html_url": "https://github.com/snnn",
            "followers_url": "https://api.github.com/users/snnn/followers",
            "following_url": "https://api.github.com/users/snnn/following{/other_user}",
            "gists_url": "https://api.github.com/users/snnn/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/snnn/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/snnn/subscriptions",
            "organizations_url": "https://api.github.com/users/snnn/orgs",
            "repos_url": "https://api.github.com/users/snnn/repos",
            "events_url": "https://api.github.com/users/snnn/events{/privacy}",
            "received_events_url": "https://api.github.com/users/snnn/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-02-22T16:23:50Z",
        "updated_at": "2022-02-22T16:23:50Z",
        "author_association": "MEMBER",
        "body": "@[skottmckay](https://github.com/skottmckay) , why the onnx line takes so much difference?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1047971456/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1048231420",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10608#issuecomment-1048231420",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10608",
        "id": 1048231420,
        "node_id": "IC_kwDOCVq1mM4-er38",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-02-22T21:31:41Z",
        "updated_at": "2022-02-22T21:31:41Z",
        "author_association": "MEMBER",
        "body": "The onnx load is just loading the protobuf data afaik - no changes to the model and access is via the protobuf types. \r\n\r\nWhen ORT loads the model we run all our optimizers, convert to ORT internal data types, potentially do pre-packing of weights, create the allocation plan, etc. etc., so I would say it's expected that `onnx.load` is a lot faster than creating the ORT InferenceSession.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1048231420/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1048287676",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10608#issuecomment-1048287676",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10608",
        "id": 1048287676,
        "node_id": "IC_kwDOCVq1mM4-e5m8",
        "user": {
            "login": "RyanUnderhill",
            "id": 38674843,
            "node_id": "MDQ6VXNlcjM4Njc0ODQz",
            "avatar_url": "https://avatars.githubusercontent.com/u/38674843?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/RyanUnderhill",
            "html_url": "https://github.com/RyanUnderhill",
            "followers_url": "https://api.github.com/users/RyanUnderhill/followers",
            "following_url": "https://api.github.com/users/RyanUnderhill/following{/other_user}",
            "gists_url": "https://api.github.com/users/RyanUnderhill/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/RyanUnderhill/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/RyanUnderhill/subscriptions",
            "organizations_url": "https://api.github.com/users/RyanUnderhill/orgs",
            "repos_url": "https://api.github.com/users/RyanUnderhill/repos",
            "events_url": "https://api.github.com/users/RyanUnderhill/events{/privacy}",
            "received_events_url": "https://api.github.com/users/RyanUnderhill/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-02-22T22:52:57Z",
        "updated_at": "2022-02-22T22:54:09Z",
        "author_association": "MEMBER",
        "body": "There's also the case where CUDA JIT will generate architecture specific code for all of the CUDA files, if we haven't prebuilt for that architecture. Typically this only happens once and is cached but we ran into cases where the cache was regenerated each time run. It was on the order of a couple of minutes when it happened.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1048287676/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1048336250",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10608#issuecomment-1048336250",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10608",
        "id": 1048336250,
        "node_id": "IC_kwDOCVq1mM4-fFd6",
        "user": {
            "login": "askhade",
            "id": 6475296,
            "node_id": "MDQ6VXNlcjY0NzUyOTY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6475296?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/askhade",
            "html_url": "https://github.com/askhade",
            "followers_url": "https://api.github.com/users/askhade/followers",
            "following_url": "https://api.github.com/users/askhade/following{/other_user}",
            "gists_url": "https://api.github.com/users/askhade/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/askhade/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/askhade/subscriptions",
            "organizations_url": "https://api.github.com/users/askhade/orgs",
            "repos_url": "https://api.github.com/users/askhade/repos",
            "events_url": "https://api.github.com/users/askhade/events{/privacy}",
            "received_events_url": "https://api.github.com/users/askhade/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-02-23T00:20:39Z",
        "updated_at": "2022-02-23T00:20:39Z",
        "author_association": "MEMBER",
        "body": "You can use cuobjdump util to find out which gpu arch the lib was compiled for: Something like :\r\ncuobjdump -sass libonnxruntime_providers_cuda.so  | grep \"arch =\"\r\n\r\nIf the lib was not compiled for your gpu's arch then I believe it is the jit compilation that is taking time.\r\n\r\nThis utility is available on windows as well : https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1048336250/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1048340621",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10608#issuecomment-1048340621",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10608",
        "id": 1048340621,
        "node_id": "IC_kwDOCVq1mM4-fGiN",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-02-23T00:28:30Z",
        "updated_at": "2022-02-23T00:28:30Z",
        "author_association": "MEMBER",
        "body": "Side note on usage. In the latest version of ORT (1.10) you need to explicitly specify that the CUDA execution provider is used via the 'providers' argument when creating the inference session. Keep that in mind if you test with 1.10 and see quicker times for the first `run` and slower times for the subsequent ones due to it only using the CPU execution provider by default. ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1048340621/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1049685287",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10608#issuecomment-1049685287",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10608",
        "id": 1049685287,
        "node_id": "IC_kwDOCVq1mM4-kO0n",
        "user": {
            "login": "Z-XQ",
            "id": 37925249,
            "node_id": "MDQ6VXNlcjM3OTI1MjQ5",
            "avatar_url": "https://avatars.githubusercontent.com/u/37925249?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Z-XQ",
            "html_url": "https://github.com/Z-XQ",
            "followers_url": "https://api.github.com/users/Z-XQ/followers",
            "following_url": "https://api.github.com/users/Z-XQ/following{/other_user}",
            "gists_url": "https://api.github.com/users/Z-XQ/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Z-XQ/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Z-XQ/subscriptions",
            "organizations_url": "https://api.github.com/users/Z-XQ/orgs",
            "repos_url": "https://api.github.com/users/Z-XQ/repos",
            "events_url": "https://api.github.com/users/Z-XQ/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Z-XQ/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-02-24T10:02:22Z",
        "updated_at": "2022-02-24T10:02:22Z",
        "author_association": "NONE",
        "body": "> You can use cuobjdump util to find out which gpu arch the lib was compiled for: Something like : cuobjdump -sass libonnxruntime_providers_cuda.so | grep \"arch =\"\r\n> \r\n> If the lib was not compiled for your gpu's arch then I believe it is the jit compilation that is taking time.\r\n> \r\n> This utility is available on windows as well : https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html\r\n\r\n@askhade Thank you! but it sounds complicated for me! I tried to solve the problem for three days, and failed. I am going to give up onnxruntime deployment and try opencv::dnn instead.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1049685287/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1050184295",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10608#issuecomment-1050184295",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10608",
        "id": 1050184295,
        "node_id": "IC_kwDOCVq1mM4-mIpn",
        "user": {
            "login": "snnn",
            "id": 856316,
            "node_id": "MDQ6VXNlcjg1NjMxNg==",
            "avatar_url": "https://avatars.githubusercontent.com/u/856316?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/snnn",
            "html_url": "https://github.com/snnn",
            "followers_url": "https://api.github.com/users/snnn/followers",
            "following_url": "https://api.github.com/users/snnn/following{/other_user}",
            "gists_url": "https://api.github.com/users/snnn/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/snnn/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/snnn/subscriptions",
            "organizations_url": "https://api.github.com/users/snnn/orgs",
            "repos_url": "https://api.github.com/users/snnn/repos",
            "events_url": "https://api.github.com/users/snnn/events{/privacy}",
            "received_events_url": "https://api.github.com/users/snnn/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-02-24T19:21:57Z",
        "updated_at": "2022-02-24T19:21:57Z",
        "author_association": "MEMBER",
        "body": "All machine learning runtimes have the same issue. By default each package on pypi has a size limit of 100MB. So each package can only include very a few CUDA ARCHs(CARDS) in its supported list. Even the maintainers can go to pypi community and ask increasing the limit, they still will not be able to include all ARCHs, it will make the packages too large.  So, if another runtime has native support for your GPU cards while ORT doesn't, it doesn't mean something.  When you get a new card or a new release from them, the situation could be different. \r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1050184295/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1050710177",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10608#issuecomment-1050710177",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10608",
        "id": 1050710177,
        "node_id": "IC_kwDOCVq1mM4-oJCh",
        "user": {
            "login": "Z-XQ",
            "id": 37925249,
            "node_id": "MDQ6VXNlcjM3OTI1MjQ5",
            "avatar_url": "https://avatars.githubusercontent.com/u/37925249?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Z-XQ",
            "html_url": "https://github.com/Z-XQ",
            "followers_url": "https://api.github.com/users/Z-XQ/followers",
            "following_url": "https://api.github.com/users/Z-XQ/following{/other_user}",
            "gists_url": "https://api.github.com/users/Z-XQ/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Z-XQ/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Z-XQ/subscriptions",
            "organizations_url": "https://api.github.com/users/Z-XQ/orgs",
            "repos_url": "https://api.github.com/users/Z-XQ/repos",
            "events_url": "https://api.github.com/users/Z-XQ/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Z-XQ/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-02-25T10:02:46Z",
        "updated_at": "2022-02-25T10:02:46Z",
        "author_association": "NONE",
        "body": "> All machine learning runtimes have the same issue. By default each package on pypi has a size limit of 100MB. So each package can only include very a few CUDA ARCHs(CARDS) in its supported list. Even the maintainers can go to pypi community and ask increasing the limit, they still will not be able to include all ARCHs, it will make the packages too large. So, if another runtime has native support for your GPU cards while ORT doesn't, it doesn't mean something. When you get a new card or a new release from them, the situation could be different.\r\n\r\nyes! you are right. My first gpu card version is 2080ti with compute ability 7.5. It takes about 0.1s to process 512x512pixl image. However, if I run the same code and model in another gpu card 3090 with compute ablility 8.6, the processing will be very slow.\r\nSo, the onnx model maybe can run normally in different gpu cards with same compute ability.  ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1050710177/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1051231016",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10608#issuecomment-1051231016",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10608",
        "id": 1051231016,
        "node_id": "IC_kwDOCVq1mM4-qIMo",
        "user": {
            "login": "askhade",
            "id": 6475296,
            "node_id": "MDQ6VXNlcjY0NzUyOTY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6475296?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/askhade",
            "html_url": "https://github.com/askhade",
            "followers_url": "https://api.github.com/users/askhade/followers",
            "following_url": "https://api.github.com/users/askhade/following{/other_user}",
            "gists_url": "https://api.github.com/users/askhade/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/askhade/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/askhade/subscriptions",
            "organizations_url": "https://api.github.com/users/askhade/orgs",
            "repos_url": "https://api.github.com/users/askhade/repos",
            "events_url": "https://api.github.com/users/askhade/events{/privacy}",
            "received_events_url": "https://api.github.com/users/askhade/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-02-25T20:27:40Z",
        "updated_at": "2022-02-25T20:27:40Z",
        "author_association": "MEMBER",
        "body": "The penalty incurred to do jit compilation for the specific cuda capability is only during cuda initialization and then subsequent runs dont incur the penalty. You can also enable caching so that you dont incur the penalty when you initialize cuda in a new process.\r\nThis is the env variable to control caching: CUDA_CACHE_DISABLE",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1051231016/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]