[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1095938429",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10919#issuecomment-1095938429",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10919",
        "id": 1095938429,
        "node_id": "IC_kwDOCVq1mM5BUrF9",
        "user": {
            "login": "faxu",
            "id": 20780999,
            "node_id": "MDQ6VXNlcjIwNzgwOTk5",
            "avatar_url": "https://avatars.githubusercontent.com/u/20780999?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/faxu",
            "html_url": "https://github.com/faxu",
            "followers_url": "https://api.github.com/users/faxu/followers",
            "following_url": "https://api.github.com/users/faxu/following{/other_user}",
            "gists_url": "https://api.github.com/users/faxu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/faxu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/faxu/subscriptions",
            "organizations_url": "https://api.github.com/users/faxu/orgs",
            "repos_url": "https://api.github.com/users/faxu/repos",
            "events_url": "https://api.github.com/users/faxu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/faxu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-04-12T03:40:31Z",
        "updated_at": "2022-04-12T03:40:31Z",
        "author_association": "MEMBER",
        "body": "Are you using onnxmltools to convert fp32 to fp16?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1095938429/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1140309860",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10919#issuecomment-1140309860",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10919",
        "id": 1140309860,
        "node_id": "IC_kwDOCVq1mM5D979k",
        "user": {
            "login": "prashantskit",
            "id": 101550102,
            "node_id": "U_kgDOBg2IFg",
            "avatar_url": "https://avatars.githubusercontent.com/u/101550102?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/prashantskit",
            "html_url": "https://github.com/prashantskit",
            "followers_url": "https://api.github.com/users/prashantskit/followers",
            "following_url": "https://api.github.com/users/prashantskit/following{/other_user}",
            "gists_url": "https://api.github.com/users/prashantskit/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/prashantskit/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/prashantskit/subscriptions",
            "organizations_url": "https://api.github.com/users/prashantskit/orgs",
            "repos_url": "https://api.github.com/users/prashantskit/repos",
            "events_url": "https://api.github.com/users/prashantskit/events{/privacy}",
            "received_events_url": "https://api.github.com/users/prashantskit/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-28T18:26:24Z",
        "updated_at": "2022-05-30T06:20:05Z",
        "author_association": "NONE",
        "body": "@soundarthiaga , The solution you mention is specific to transformers. Can you suggest what I should do rectify my problem?\r\n\r\nHi, I am also experiencing the same issue. The performance of fp16 is slower in comparison to fp32 by significant amount on CPU. \r\n\r\nDescription:\r\nI am converting tacotron2 model to onnx using this link [onnx-export](https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/SpeechSynthesis/Tacotron2/exports/export_tacotron2_onnx.py)\r\nFramework - Pytorch version 1.10.2\r\nExporing method - torch.onnx.export\r\nopset - 11\r\nonnx version - 1.11.0\r\nOS - Ubuntu 20.04\r\nProcessor - Intel Xeon\r\nInference is done using onnxruntime\r\nonnx runtime version - 1.11.1\r\n\r\nI used convert_float_to_float16 function from onnxconverter_common [quantisation](https://github.com/microsoft/onnxconverter-common/blob/master/onnxconverter_common/float16.py) input model is onnx float 32 and expected output is float16.\r\n\r\n```\r\nimport onnx\r\nfrom onnxconverter_common.float16 import convert_float_to_float16\r\n\r\nmodel_fp32_decoder = 'tacotron2/outdir/working-onnx/decoder_iter.onnx'\r\nmodel_quant_decoder = 'tacotron2/outdir/working-onnx/decoder_iter.fp16.onnx'\r\nmodel = onnx.load_model(model_fp32_decoder)\r\nnew_model = convert_float_to_float16(model)\r\n\r\nonnx.save_model(new_model, model_quant_decoder) \r\n```\r\n\r\nIn one of the issue I read fp16 is not supported in Arm but I am using Intel. Can somebody help with the issue? Let me know  if more information is needed.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1140309860/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]