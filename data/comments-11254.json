[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1105488598",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/11254#issuecomment-1105488598",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/11254",
        "id": 1105488598,
        "node_id": "IC_kwDOCVq1mM5B5GrW",
        "user": {
            "login": "hariharans29",
            "id": 9969784,
            "node_id": "MDQ6VXNlcjk5Njk3ODQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9969784?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hariharans29",
            "html_url": "https://github.com/hariharans29",
            "followers_url": "https://api.github.com/users/hariharans29/followers",
            "following_url": "https://api.github.com/users/hariharans29/following{/other_user}",
            "gists_url": "https://api.github.com/users/hariharans29/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hariharans29/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hariharans29/subscriptions",
            "organizations_url": "https://api.github.com/users/hariharans29/orgs",
            "repos_url": "https://api.github.com/users/hariharans29/repos",
            "events_url": "https://api.github.com/users/hariharans29/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hariharans29/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-04-21T17:18:10Z",
        "updated_at": "2022-04-22T00:39:13Z",
        "author_association": "MEMBER",
        "body": "I believe I know the reason for this redundant round trip. I will investigate it to confirm it but I think it is because a gap in our location planning for certain kinds of inputs (specficially a graph input that is consumed not in the main graph but in the subgraph - like in your case, where the input image is only consumed within the If subgraphs). Due to this gap in the planning, IOBinding thinks that the input is \"required\" on CPU which causes the first part of the round-trip. The second part is because the data itself is required on CUDA in the subgraph and this causes the copy from CPU to CUDA when the If executes. \r\n\r\n\r\nSome follow-up questions:\r\n\r\n1) If the round-trip copy is fixed, are you sure the latency will improve to the point where your model(s) can be shipped ?\r\n\r\n2) Are you able to build from source to validate a fix ?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1105488598/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1105502871",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/11254#issuecomment-1105502871",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/11254",
        "id": 1105502871,
        "node_id": "IC_kwDOCVq1mM5B5KKX",
        "user": {
            "login": "admayber",
            "id": 44822281,
            "node_id": "MDQ6VXNlcjQ0ODIyMjgx",
            "avatar_url": "https://avatars.githubusercontent.com/u/44822281?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/admayber",
            "html_url": "https://github.com/admayber",
            "followers_url": "https://api.github.com/users/admayber/followers",
            "following_url": "https://api.github.com/users/admayber/following{/other_user}",
            "gists_url": "https://api.github.com/users/admayber/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/admayber/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/admayber/subscriptions",
            "organizations_url": "https://api.github.com/users/admayber/orgs",
            "repos_url": "https://api.github.com/users/admayber/repos",
            "events_url": "https://api.github.com/users/admayber/events{/privacy}",
            "received_events_url": "https://api.github.com/users/admayber/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-04-21T17:35:36Z",
        "updated_at": "2022-04-21T17:35:36Z",
        "author_association": "MEMBER",
        "body": "@hariharans29 thanks for your response, yeah that makes sense as a probable cause. Per your questions:\r\n\r\n1. Very much so. Memory operation time is currently overwhelming the inference run time because we are making a lot of repeated inference calls against the same (fairly large) image with slightly different parameters each time\r\n2. Yes, I've done source builds recently so this should be no issue at all (might need guidance on the specific parameters to use)",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1105502871/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1108650749",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/11254#issuecomment-1108650749",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/11254",
        "id": 1108650749,
        "node_id": "IC_kwDOCVq1mM5CFKr9",
        "user": {
            "login": "pommedeterresautee",
            "id": 1029874,
            "node_id": "MDQ6VXNlcjEwMjk4NzQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1029874?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pommedeterresautee",
            "html_url": "https://github.com/pommedeterresautee",
            "followers_url": "https://api.github.com/users/pommedeterresautee/followers",
            "following_url": "https://api.github.com/users/pommedeterresautee/following{/other_user}",
            "gists_url": "https://api.github.com/users/pommedeterresautee/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pommedeterresautee/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pommedeterresautee/subscriptions",
            "organizations_url": "https://api.github.com/users/pommedeterresautee/orgs",
            "repos_url": "https://api.github.com/users/pommedeterresautee/repos",
            "events_url": "https://api.github.com/users/pommedeterresautee/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pommedeterresautee/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-04-25T14:28:18Z",
        "updated_at": "2022-04-25T14:28:18Z",
        "author_association": "NONE",
        "body": "I am experiencing the very same issue than described above.\r\n\r\nI have an ONNX model with a `If` node at the very beginning of its computation graph, in the then branch there is a transformer (decoder) computation graph which supports caching and in the else branch the same model without caching support.\r\n\r\nThe output of the ONNX model are ok, but the computation is 3-4X slower when I am using the model with the If node + 2 subgraphs Vs using any of the subgraph directly.\r\n\r\nThe `If` node is conditioned on some scalar bool input.\r\nInputs are sent to Onnx Runtime through io binding API.\r\n\r\nCode is available there: https://github.com/ELS-RD/transformer-deploy/blob/t5/t5.ipynb\r\n\r\nI tested the fix you are working on (https://github.com/microsoft/onnxruntime/pull/11320) but it didn't brought any speed improvement compared to ORT 1.11 :-(\r\n\r\n```shell\r\ngit clone --recursive https://github.com/Microsoft/onnxruntime\r\ncd onnxruntime\r\ngit fetch origin hari/location_plan_implicit_inputs\r\ngit checkout -b hari/location_plan_implicit_inputs FETCH_HEAD\r\nCUDACXX=/usr/local/cuda-11.5/bin/nvcc ./build.sh \\\r\n    --config Release \\\r\n    --build_wheel \\\r\n    --parallel \\\r\n    --use_cuda \\\r\n    --cuda_home /usr/local/cuda-11.5 \\\r\n    --cudnn_home /usr/lib/x86_64-linux-gnu/ \\\r\n    --skip_test\r\n```\r\n\r\n@hariharans29 Am I supposed to see an improvement in speed with https://github.com/microsoft/onnxruntime/pull/11320, or will there be several PR related to this issue?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1108650749/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1108778295",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/11254#issuecomment-1108778295",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/11254",
        "id": 1108778295,
        "node_id": "IC_kwDOCVq1mM5CFp03",
        "user": {
            "login": "hariharans29",
            "id": 9969784,
            "node_id": "MDQ6VXNlcjk5Njk3ODQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9969784?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hariharans29",
            "html_url": "https://github.com/hariharans29",
            "followers_url": "https://api.github.com/users/hariharans29/followers",
            "following_url": "https://api.github.com/users/hariharans29/following{/other_user}",
            "gists_url": "https://api.github.com/users/hariharans29/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hariharans29/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hariharans29/subscriptions",
            "organizations_url": "https://api.github.com/users/hariharans29/orgs",
            "repos_url": "https://api.github.com/users/hariharans29/repos",
            "events_url": "https://api.github.com/users/hariharans29/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hariharans29/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-04-25T16:17:48Z",
        "updated_at": "2022-04-25T16:20:39Z",
        "author_association": "MEMBER",
        "body": "@[pommedeterresautee](https://github.com/pommedeterresautee) : This is the only PR I have at the moment and it is trying to solve a very specific issue of having an If node at the top of the graph consuming graph inputs that is cauing round-trip copies in the OP's case. Maybe there is a different issue or combinations of multiple issues surfacing from your use-case of having an If node - it is hard to tell without inspecting the graph. I am waiting on the OP to confirm that their issue was fixed by #11320 and if it is, unfortunately, that is the only fix I have for now. \r\n\r\nI suggest opening a different issue with more details (a minimal ONNX model to repro at the very least) and show how you are using the ORT APIs for someone on the team to make an assessment. ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1108778295/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1108858289",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/11254#issuecomment-1108858289",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/11254",
        "id": 1108858289,
        "node_id": "IC_kwDOCVq1mM5CF9Wx",
        "user": {
            "login": "pommedeterresautee",
            "id": 1029874,
            "node_id": "MDQ6VXNlcjEwMjk4NzQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1029874?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pommedeterresautee",
            "html_url": "https://github.com/pommedeterresautee",
            "followers_url": "https://api.github.com/users/pommedeterresautee/followers",
            "following_url": "https://api.github.com/users/pommedeterresautee/following{/other_user}",
            "gists_url": "https://api.github.com/users/pommedeterresautee/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pommedeterresautee/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pommedeterresautee/subscriptions",
            "organizations_url": "https://api.github.com/users/pommedeterresautee/orgs",
            "repos_url": "https://api.github.com/users/pommedeterresautee/repos",
            "events_url": "https://api.github.com/users/pommedeterresautee/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pommedeterresautee/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-04-25T17:43:14Z",
        "updated_at": "2022-04-25T20:49:14Z",
        "author_association": "NONE",
        "body": "I understand, thank you for your answer, I will wait the OP and if her bug is fixed, I will reopen an issue.\r\n\r\nIn between, here is an nsight screenshot were we can see that in model with If node, most of the time is spent on memory transfers between host and device:\r\n\r\nModel with If node:\r\n\r\n![image](https://user-images.githubusercontent.com/1029874/165143774-74d82abf-c048-45c5-84e2-b48c19471905.png)\r\n\r\n> red and green rectangles are memory transfer between host and GPU, blue is for kernel exec\r\n\r\nVery same subgraph but no If node on top of it:\r\n\r\n![image](https://user-images.githubusercontent.com/1029874/165160736-8caa921e-8f50-4dfc-8497-8192b399d572.png)\r\n\r\n\r\nIn the inference, I call the model 100 times, the input is 3 torch tensors created 1 time only and reused for each inference.\r\ninput torch tensor pointers are provided to IO binding Python ORT API.\r\nOnly output tensor is reallocated from 1 inference call to the next.\r\nThe onnxruntime-gpu used is the one compiled from the branch `hari/location_plan_implicit_inputs`.\r\nGCC 10.3 is used to be compatible with nsight.\r\n\r\n```python\r\n#  Copyright 2022, Lefebvre Dalloz Services\r\n#\r\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\r\n#  you may not use this file except in compliance with the License.\r\n#  You may obtain a copy of the License at\r\n#\r\n#    http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n#  Unless required by applicable law or agreed to in writing, software\r\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\r\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n#  See the License for the specific language governing permissions and\r\n#  limitations under the License.\r\n\r\nfrom typing import Dict, Optional, Tuple, Union\r\n\r\nimport numpy as np\r\nimport onnx\r\nimport torch\r\nfrom onnx import GraphProto, ModelProto, helper\r\nfrom onnxruntime import ExecutionMode, GraphOptimizationLevel, InferenceSession, IOBinding, SessionOptions\r\n\r\n\r\nnumpy_to_torch_dtype_dict = {\r\n    bool: torch.bool,\r\n    np.uint8: torch.uint8,\r\n    np.int8: torch.int8,\r\n    np.int16: torch.int16,\r\n    np.int32: torch.int32,\r\n    np.int64: torch.int64,\r\n    np.float16: torch.float16,\r\n    np.float32: torch.float32,\r\n    np.float64: torch.float64,\r\n    np.complex64: torch.complex64,\r\n    np.complex128: torch.complex128,\r\n}\r\ntorch_to_numpy_dtype_dict = {v: k for k, v in numpy_to_torch_dtype_dict.items()}\r\n\r\n\r\ndef inference_onnx_binding(\r\n    model_onnx: InferenceSession,\r\n    inputs: Dict[str, torch.Tensor],\r\n    device: str,\r\n    output_shape: Optional[Union[Tuple[int], Dict[str, Tuple[int]]]] = None,\r\n    device_id: int = 0,\r\n) -> Dict[str, torch.Tensor]:\r\n    \"\"\"\r\n    Performs inference on ONNX Runtime in an optimized way.\r\n    In particular, avoid some tensor copy from GPU to host by using Torch tensors directly.\r\n\r\n    :param model_onnx: ONNX model\r\n    :param inputs: input torch tensor\r\n    :param device: where to run the inference. One of [cpu, cuda]\r\n    :param output_shape: tensor output shape if known, otherwise will be guessed from axis names\r\n    :param device_id: ID of the device where to run the inference, to be used when there are multiple GPUs, etc.\r\n    :return: a dict {axis name: output tensor}\r\n    \"\"\"\r\n    assert device in [\"cpu\", \"cuda\"]\r\n    # assert len(inputs) == len(model_onnx.get_inputs())\r\n    binding: IOBinding = model_onnx.io_binding()\r\n    for input_onnx in model_onnx.get_inputs():\r\n        if input_onnx.name not in inputs:  # some inputs may be optional\r\n            continue\r\n        tensor: torch.Tensor = inputs[input_onnx.name]\r\n        tensor = tensor.contiguous()\r\n        if tensor.dtype in [torch.int64, torch.long]:\r\n            # int32 mandatory as input of bindings, int64 not supported\r\n            tensor = tensor.type(dtype=torch.int32).to(device)\r\n        binding.bind_input(\r\n            name=input_onnx.name,\r\n            device_type=device,\r\n            device_id=device_id,\r\n            element_type=torch_to_numpy_dtype_dict[tensor.dtype],\r\n            shape=tuple(tensor.shape),\r\n            buffer_ptr=tensor.data_ptr(),\r\n        )\r\n        inputs[input_onnx.name] = tensor\r\n    outputs = dict()\r\n    for output_name, shape in output_shape.items():\r\n        tensor = torch.empty(shape, dtype=torch.float32, device=device).contiguous()\r\n        outputs[output_name] = tensor\r\n        binding.bind_output(\r\n            name=output_name,\r\n            device_type=device,\r\n            device_id=device_id,\r\n            element_type=np.float32,  # hard coded output type\r\n            shape=tuple(shape),\r\n            buffer_ptr=tensor.data_ptr(),\r\n        )\r\n    model_onnx.run_with_iobinding(binding)\r\n    return outputs\r\n\r\n\r\nonnx_model_cache = onnx.load(\"test-dec-cache.onnx\")\r\nonnx_model_no_cache = onnx.load(\"test-dec-no-cache.onnx\")\r\n\r\n\r\nprefix = \"cache_node_\"\r\nmapping_initializer_cache_to_no_cache = dict()\r\nto_add = list()\r\nfor node_cache in onnx_model_cache.graph.initializer:\r\n    found = False\r\n    for node_no_cache in onnx_model_no_cache.graph.initializer:\r\n        if node_cache.raw_data == node_no_cache.raw_data:\r\n            found = True\r\n            mapping_initializer_cache_to_no_cache[node_cache.name] = node_no_cache.name\r\n            break\r\n    if not found:\r\n        node_cache.name = prefix + node_cache.name\r\n        to_add.append(node_cache)\r\n        mapping_initializer_cache_to_no_cache[node_cache.name] = node_cache.name\r\n        print(f\"name: {node_cache.name} - size: {len(node_cache.raw_data)/1024:.2f}\")\r\n\r\nonnx_model_no_cache.graph.initializer.extend(to_add)\r\n# I/O model names should not be prefixed\r\nmodel_io_names = [n.name for n in list(onnx_model_cache.graph.input) + list(onnx_model_cache.graph.output)]\r\n\r\nfor node in onnx_model_cache.graph.node:\r\n    for index, input_name in enumerate(node.input):\r\n        if input_name in model_io_names:\r\n            continue\r\n        node.input[index] = mapping_initializer_cache_to_no_cache.get(input_name, prefix + input_name)\r\n    for index, output_name in enumerate(node.output):\r\n        if output_name in model_io_names:\r\n            continue\r\n        node.output[index] = prefix + output_name\r\n    node.name = prefix + node.name\r\nmodel_io_names = [n.name for n in list(onnx_model_cache.graph.input) + list(onnx_model_cache.graph.output)]\r\n\r\nprefix = \"init_\"\r\ncache = dict()\r\nfor node in onnx_model_no_cache.graph.initializer:\r\n    if node.name in model_io_names:\r\n        new_name = prefix + node.name\r\n        cache[node.name] = new_name\r\n        node.name = new_name\r\n\r\nfor node in onnx_model_no_cache.graph.node:\r\n    for index, n in enumerate(node.input):\r\n        node.input[index] = cache.get(n, n)\r\n\r\n# mandatory for subgraph in if/else node\r\nassert len(onnx_model_cache.graph.output) == len(onnx_model_no_cache.graph.output)\r\n\r\ngraph_cache: onnx.GraphProto = onnx.helper.make_graph(\r\n    nodes=list(onnx_model_cache.graph.node),\r\n    name=\"graph-cache\",\r\n    inputs=[],\r\n    outputs=list(onnx_model_cache.graph.output),\r\n    initializer=[],\r\n)\r\n\r\ngraph_no_cache: onnx.GraphProto = onnx.helper.make_graph(\r\n    nodes=list(onnx_model_no_cache.graph.node),\r\n    name=\"graph-no-cache\",\r\n    inputs=[],\r\n    outputs=list(onnx_model_no_cache.graph.output),\r\n    initializer=[],\r\n)\r\n\r\nenable_cache = onnx.helper.make_tensor_value_info(name=\"enable_cache\", elem_type=onnx.TensorProto.BOOL, shape=[1])\r\n\r\nif_node = onnx.helper.make_node(\r\n    op_type=\"If\",\r\n    inputs=[\"enable_cache\"],\r\n    outputs=[o.name for o in list(onnx_model_no_cache.graph.output)],\r\n    then_branch=graph_cache,\r\n    else_branch=graph_no_cache,\r\n)\r\n\r\nif_graph_def: GraphProto = helper.make_graph(\r\n    nodes=[if_node],\r\n    name=\"if-model\",\r\n    inputs=list(onnx_model_cache.graph.input) + [enable_cache],\r\n    outputs=list(onnx_model_no_cache.graph.output),\r\n    initializer=list(onnx_model_no_cache.graph.initializer),\r\n)\r\n\r\nmodel_def: ModelProto = helper.make_model(if_graph_def, producer_name=\"onnx-example\")\r\n\r\nonnx.checker.check_model(model_def)\r\n\r\noptions = SessionOptions()\r\noptions.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL\r\nort_cache = InferenceSession(model_def.SerializeToString(), options, providers=[\"CUDAExecutionProvider\"])\r\n\r\nbatch = 4\r\nseq_len = 1000\r\ninput_cache = dict()\r\ninput_cache[\"input_ids\"] = torch.ones((batch, seq_len), dtype=torch.int32, device=\"cuda\")\r\ninput_cache[\"encoder_hidden_states\"] = torch.ones((batch, seq_len, 512), dtype=torch.float32, device=\"cuda\")\r\ninput_cache[\"enable_cache\"] = torch.tensor([False], device=\"cuda\", dtype=torch.bool)\r\n\r\nfor _ in range(100):\r\n    _ = inference_onnx_binding(\r\n        model_onnx=ort_cache, inputs=input_cache, device=\"cuda\", output_shape={\"logits\": (batch, seq_len, 32128)}\r\n    )\r\n\r\nprint(\"finished\")\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1108858289/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1109416276",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/11254#issuecomment-1109416276",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/11254",
        "id": 1109416276,
        "node_id": "IC_kwDOCVq1mM5CIFlU",
        "user": {
            "login": "pommedeterresautee",
            "id": 1029874,
            "node_id": "MDQ6VXNlcjEwMjk4NzQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1029874?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pommedeterresautee",
            "html_url": "https://github.com/pommedeterresautee",
            "followers_url": "https://api.github.com/users/pommedeterresautee/followers",
            "following_url": "https://api.github.com/users/pommedeterresautee/following{/other_user}",
            "gists_url": "https://api.github.com/users/pommedeterresautee/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pommedeterresautee/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pommedeterresautee/subscriptions",
            "organizations_url": "https://api.github.com/users/pommedeterresautee/orgs",
            "repos_url": "https://api.github.com/users/pommedeterresautee/repos",
            "events_url": "https://api.github.com/users/pommedeterresautee/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pommedeterresautee/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-04-26T06:54:05Z",
        "updated_at": "2022-04-26T06:54:05Z",
        "author_association": "NONE",
        "body": "@hariharans29 please find the 2 models used in the script above in these GDrive links:\r\n- ONNX model (decoder with cache): https://drive.google.com/file/d/1M3i3g-2ib8BZpu64INqI98Kw5pHY6uaZ/view?usp=sharing\r\n- ONNX model (decoder without cache): https://drive.google.com/file/d/1-JTgAcTh-2D9QGzIFOeorcsals6tzJwc/view?usp=sharing\r\n\r\nThey are the decoder part of `T5` small flavor (from hugging face hub) exported by Pytorch to ONNX without any modification/optimization.\r\n\r\nAs explained above, the two model computation graphs are \"merged\" in a single one through a `If` node.\r\nMost of the ONNX code in the script above is to manage collisions between node/axis/weight names.\r\n\r\nThe goal is to support cache/no cache setup on autoregressive models without duplicating weights (which is the usual way of doing inference on T5 when using ONNX like in https://github.com/Ki6an/fastT5). FWIW no model from Hugging Face library supports JIT script export so only tracing is possible.\r\n\r\nMerged model outputs has been checked (compared with Pytorch output) with real data and everything is working as expected.\r\n\r\n`If` node consumes only one input (a `bool` called \"enable_cache\") and all the other inputs are only consumed by the subgraphs (there are 27 inputs in the merged model). One subgraph has more input than the other (past state for the K/Vcache) which is similar to the case described in the PR and by the OP.\r\n\r\nI edited my previous message so the script has no dependency outside onnx / onnxruntime / pytorch / numpy.\r\n\r\nLet me know if somethings else is needed to hep for reproducibility.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1109416276/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1110023627",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/11254#issuecomment-1110023627",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/11254",
        "id": 1110023627,
        "node_id": "IC_kwDOCVq1mM5CKZ3L",
        "user": {
            "login": "admayber",
            "id": 44822281,
            "node_id": "MDQ6VXNlcjQ0ODIyMjgx",
            "avatar_url": "https://avatars.githubusercontent.com/u/44822281?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/admayber",
            "html_url": "https://github.com/admayber",
            "followers_url": "https://api.github.com/users/admayber/followers",
            "following_url": "https://api.github.com/users/admayber/following{/other_user}",
            "gists_url": "https://api.github.com/users/admayber/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/admayber/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/admayber/subscriptions",
            "organizations_url": "https://api.github.com/users/admayber/orgs",
            "repos_url": "https://api.github.com/users/admayber/repos",
            "events_url": "https://api.github.com/users/admayber/events{/privacy}",
            "received_events_url": "https://api.github.com/users/admayber/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-04-26T16:46:01Z",
        "updated_at": "2022-04-26T16:46:01Z",
        "author_association": "MEMBER",
        "body": "@hariharans29 for my part, it looks like the fix in https://github.com/microsoft/onnxruntime/pull/11320 addressed the issue I was having, I'm no longer seeing extraneous round-trip memory copies and inference time has dropped dramatically as expected",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1110023627/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1110077240",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/11254#issuecomment-1110077240",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/11254",
        "id": 1110077240,
        "node_id": "IC_kwDOCVq1mM5CKm84",
        "user": {
            "login": "hariharans29",
            "id": 9969784,
            "node_id": "MDQ6VXNlcjk5Njk3ODQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9969784?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hariharans29",
            "html_url": "https://github.com/hariharans29",
            "followers_url": "https://api.github.com/users/hariharans29/followers",
            "following_url": "https://api.github.com/users/hariharans29/following{/other_user}",
            "gists_url": "https://api.github.com/users/hariharans29/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hariharans29/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hariharans29/subscriptions",
            "organizations_url": "https://api.github.com/users/hariharans29/orgs",
            "repos_url": "https://api.github.com/users/hariharans29/repos",
            "events_url": "https://api.github.com/users/hariharans29/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hariharans29/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-04-26T17:42:40Z",
        "updated_at": "2022-04-26T17:43:33Z",
        "author_association": "MEMBER",
        "body": "@admayber : Thanks for the confirmation. I merged the PR now.\r\n\r\n@pommedeterresautee : Since the OP confirmed that their bug is fixed with the PR, I am guessing the issue your model is experiencing (albeit with the similar model structure with the `If` node as the OP's model) is a different one. Can you please open another issue with the details you shared here and someone will dig into it.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1110077240/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1110165237",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/11254#issuecomment-1110165237",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/11254",
        "id": 1110165237,
        "node_id": "IC_kwDOCVq1mM5CK8b1",
        "user": {
            "login": "pommedeterresautee",
            "id": 1029874,
            "node_id": "MDQ6VXNlcjEwMjk4NzQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1029874?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pommedeterresautee",
            "html_url": "https://github.com/pommedeterresautee",
            "followers_url": "https://api.github.com/users/pommedeterresautee/followers",
            "following_url": "https://api.github.com/users/pommedeterresautee/following{/other_user}",
            "gists_url": "https://api.github.com/users/pommedeterresautee/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pommedeterresautee/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pommedeterresautee/subscriptions",
            "organizations_url": "https://api.github.com/users/pommedeterresautee/orgs",
            "repos_url": "https://api.github.com/users/pommedeterresautee/repos",
            "events_url": "https://api.github.com/users/pommedeterresautee/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pommedeterresautee/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-04-26T19:23:43Z",
        "updated_at": "2022-04-26T19:23:43Z",
        "author_association": "NONE",
        "body": "Thank you @hariharans29, it's done.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1110165237/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]