[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1104530761",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/11274#issuecomment-1104530761",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/11274",
        "id": 1104530761,
        "node_id": "IC_kwDOCVq1mM5B1c1J",
        "user": {
            "login": "pranavsharma",
            "id": 2732907,
            "node_id": "MDQ6VXNlcjI3MzI5MDc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2732907?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pranavsharma",
            "html_url": "https://github.com/pranavsharma",
            "followers_url": "https://api.github.com/users/pranavsharma/followers",
            "following_url": "https://api.github.com/users/pranavsharma/following{/other_user}",
            "gists_url": "https://api.github.com/users/pranavsharma/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pranavsharma/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pranavsharma/subscriptions",
            "organizations_url": "https://api.github.com/users/pranavsharma/orgs",
            "repos_url": "https://api.github.com/users/pranavsharma/repos",
            "events_url": "https://api.github.com/users/pranavsharma/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pranavsharma/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-04-20T22:53:00Z",
        "updated_at": "2022-04-20T22:53:00Z",
        "author_association": "MEMBER",
        "body": "From the demo code it looks like you're not using iobinding and not ensuring the inputs are copied to the target device before calling Run(). See https://onnxruntime.ai/docs/api/python/api_summary.html#iobinding",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1104530761/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1104673993",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/11274#issuecomment-1104673993",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/11274",
        "id": 1104673993,
        "node_id": "IC_kwDOCVq1mM5B1_zJ",
        "user": {
            "login": "hariharans29",
            "id": 9969784,
            "node_id": "MDQ6VXNlcjk5Njk3ODQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9969784?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hariharans29",
            "html_url": "https://github.com/hariharans29",
            "followers_url": "https://api.github.com/users/hariharans29/followers",
            "following_url": "https://api.github.com/users/hariharans29/following{/other_user}",
            "gists_url": "https://api.github.com/users/hariharans29/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hariharans29/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hariharans29/subscriptions",
            "organizations_url": "https://api.github.com/users/hariharans29/orgs",
            "repos_url": "https://api.github.com/users/hariharans29/repos",
            "events_url": "https://api.github.com/users/hariharans29/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hariharans29/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-04-21T03:38:16Z",
        "updated_at": "2022-04-21T03:38:16Z",
        "author_association": "MEMBER",
        "body": "1) Can you just attach the ONNX model ? It will be easier than trying to install the necessary dependencies to then run the Python script to generate the model.\r\n\r\n2) Can you paste the warning you see when you run `DepthToSpace` ?\r\n\r\n3) Since you have shared the PyTorch kernel profiles, can you share the ORT kernel profile as well ? \r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1104673993/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1104677789",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/11274#issuecomment-1104677789",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/11274",
        "id": 1104677789,
        "node_id": "IC_kwDOCVq1mM5B2Aud",
        "user": {
            "login": "hariharans29",
            "id": 9969784,
            "node_id": "MDQ6VXNlcjk5Njk3ODQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9969784?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hariharans29",
            "html_url": "https://github.com/hariharans29",
            "followers_url": "https://api.github.com/users/hariharans29/followers",
            "following_url": "https://api.github.com/users/hariharans29/following{/other_user}",
            "gists_url": "https://api.github.com/users/hariharans29/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hariharans29/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hariharans29/subscriptions",
            "organizations_url": "https://api.github.com/users/hariharans29/orgs",
            "repos_url": "https://api.github.com/users/hariharans29/repos",
            "events_url": "https://api.github.com/users/hariharans29/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hariharans29/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-04-21T03:47:14Z",
        "updated_at": "2022-04-21T03:47:14Z",
        "author_association": "MEMBER",
        "body": "4. It seems like `run_inference()` is being timed in your demo code which includes pre-processing + post-processing + a call to `session.get_inputs()[0].name` for every inference run (which can be avoided as this info can be cached). Is your comparison with PyTorch apple-to-apple (i.e.) are you including pre-processing + post-processing + input copy to the device from CPU + output copy from the device to CPU latencies in PyTorch as well ?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1104677789/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1105028339",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/11274#issuecomment-1105028339",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/11274",
        "id": 1105028339,
        "node_id": "IC_kwDOCVq1mM5B3WTz",
        "user": {
            "login": "xiong-jie-y",
            "id": 32722944,
            "node_id": "MDQ6VXNlcjMyNzIyOTQ0",
            "avatar_url": "https://avatars.githubusercontent.com/u/32722944?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/xiong-jie-y",
            "html_url": "https://github.com/xiong-jie-y",
            "followers_url": "https://api.github.com/users/xiong-jie-y/followers",
            "following_url": "https://api.github.com/users/xiong-jie-y/following{/other_user}",
            "gists_url": "https://api.github.com/users/xiong-jie-y/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/xiong-jie-y/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/xiong-jie-y/subscriptions",
            "organizations_url": "https://api.github.com/users/xiong-jie-y/orgs",
            "repos_url": "https://api.github.com/users/xiong-jie-y/repos",
            "events_url": "https://api.github.com/users/xiong-jie-y/events{/privacy}",
            "received_events_url": "https://api.github.com/users/xiong-jie-y/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-04-21T10:27:57Z",
        "updated_at": "2022-04-21T10:27:57Z",
        "author_association": "NONE",
        "body": "@pranavsharma \r\nThank you! I will try the implementation you referred to too.\r\n\r\n@hariharans29 \r\nThank you for the clarification!\r\nThe answers are\r\n1. This is the model [imdn_256x256.zip](https://github.com/microsoft/onnxruntime/files/8529936/imdn_256x256.zip)\r\n2. The warnings are \r\n```\r\n2022-04-20 15:46:06.927163928 [W:onnxruntime:, execution_frame.cc:806 VerifyOutputSizes] Expected shape from model of {1,3,1024,1024} does not match actual shape of {1,3,256,4,256,4} for output output\r\n```\r\n3. I will try cuda kernel profiling later...please wait.\r\n4. Sorry for not sharing the measuring code. I also measured the running time including the data copy \r\n  * the cpu <-> gpu copy + model inference time in pytorch: around 18ms\r\n  * the cpu <-> gpu copy + model inference time in onnxruntime: around 21ms\r\n \r\nThe snippets I used to measure the running time is like this\r\n\r\n**For onnxruntime**\r\n```python\r\n    s = time.perf_counter()\r\n    result = onnx_session.run(None, {input_name: input_image})\r\n    print(time.perf_counter() - s)\r\n```\r\n\r\n**For pytorch**\r\nFull code is [here](https://gist.github.com/xiong-jie-y/ed1f4256b9a117db0045f85874af397b).\r\nThe model is [here](https://github.com/ofsoundof/IMDN/blob/main/model_zoo/imdn_x4.pth)\r\n```python\r\n        torch.cuda.synchronize()\r\n        s = time.perf_counter()\r\n        img_L = img_L.to(device)\r\n        img_E = model(img_L)\r\n        img_E = img_E.cpu()\r\n        torch.cuda.synchronize()\r\n        print(time.perf_counter() - s)\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1105028339/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1105813967",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/11274#issuecomment-1105813967",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/11274",
        "id": 1105813967,
        "node_id": "IC_kwDOCVq1mM5B6WHP",
        "user": {
            "login": "hariharans29",
            "id": 9969784,
            "node_id": "MDQ6VXNlcjk5Njk3ODQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9969784?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hariharans29",
            "html_url": "https://github.com/hariharans29",
            "followers_url": "https://api.github.com/users/hariharans29/followers",
            "following_url": "https://api.github.com/users/hariharans29/following{/other_user}",
            "gists_url": "https://api.github.com/users/hariharans29/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hariharans29/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hariharans29/subscriptions",
            "organizations_url": "https://api.github.com/users/hariharans29/orgs",
            "repos_url": "https://api.github.com/users/hariharans29/repos",
            "events_url": "https://api.github.com/users/hariharans29/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hariharans29/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-04-21T22:15:53Z",
        "updated_at": "2022-04-21T22:17:40Z",
        "author_association": "MEMBER",
        "body": "#11302 should fix the warning you saw on every Run().\r\n\r\nFor the perf part, after you made it apple-to-apple (include CPU<->GPU copies + inferencing latency), the gap is just 3ms (21ms - 18ms). We can see if there is any kernel whose perf we can improve after you share the kernel profiling for ORT.\r\n\r\nAnother thing to keep in mind is that for ORT, you would have to ignore the perf on the very first `Run()` as ORT sets up some things on the very first `Run()`, if you haven't done so already, can you share the average latency after ignoring the latency on the first `Run()` and possibly setting the logging level to just ERROR (to avoid the WARNING logging latency being included) ?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1105813967/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1108512661",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/11274#issuecomment-1108512661",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/11274",
        "id": 1108512661,
        "node_id": "IC_kwDOCVq1mM5CEo-V",
        "user": {
            "login": "xiong-jie-y",
            "id": 32722944,
            "node_id": "MDQ6VXNlcjMyNzIyOTQ0",
            "avatar_url": "https://avatars.githubusercontent.com/u/32722944?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/xiong-jie-y",
            "html_url": "https://github.com/xiong-jie-y",
            "followers_url": "https://api.github.com/users/xiong-jie-y/followers",
            "following_url": "https://api.github.com/users/xiong-jie-y/following{/other_user}",
            "gists_url": "https://api.github.com/users/xiong-jie-y/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/xiong-jie-y/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/xiong-jie-y/subscriptions",
            "organizations_url": "https://api.github.com/users/xiong-jie-y/orgs",
            "repos_url": "https://api.github.com/users/xiong-jie-y/repos",
            "events_url": "https://api.github.com/users/xiong-jie-y/events{/privacy}",
            "received_events_url": "https://api.github.com/users/xiong-jie-y/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-04-25T12:33:27Z",
        "updated_at": "2022-04-25T12:33:27Z",
        "author_association": "NONE",
        "body": "Thank you for fixing the warning issue!\r\n\r\nI profiled model in kernel level both on pytorch and onnxruntime.\r\n\r\n### Pytorch (per 100 run)\r\nThis is profiled like below.\r\n```python\r\n    with profile(activities=[\r\n        ProfilerActivity.CUDA], \r\n        schedule=torch.profiler.schedule(\r\n                wait=1,\r\n                warmup=2,\r\n                active=100,\r\n                repeat=1),record_shapes=True) as prof:\r\n        for iter in range(103):\r\n            img_E = model(img_L)\r\n            prof.step()\r\n```\r\n\r\n#### Profile Result\r\n```\r\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                                                                                                                                                                                Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \r\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                                                                                                                                                                    cudaLaunchKernel        92.63%        1.192s        92.63%        1.192s      69.330us       0.000us         0.00%       0.000us       0.000us         17200  \r\n                                                                                                                                                                     cudaEventRecord         0.08%       1.035ms         0.08%       1.035ms       0.241us       0.000us         0.00%       0.000us       0.000us          4300  \r\nvoid at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::AddFunctor<float> > >(at::TensorIteratorBase&,...         0.00%       0.000us         0.00%       0.000us       0.000us     305.782ms        19.46%     305.782ms      70.053us          4365  \r\nvoid at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::leaky_relu_kernel(at::TensorIteratorBase&, c10::Scalar const&)::{lambda()#1}::operator()() c...         0.00%       0.000us         0.00%       0.000us       0.000us     205.348ms        13.07%     205.348ms      84.297us          2436  \r\n                                                      void cudnn::winograd::generateWinogradTilesKernel<0, float, float>(cudnn::winograd::GenerateWinogradTilesParams<float, float>)         0.00%       0.000us         0.00%       0.000us       0.000us      10.078ms         0.64%      10.078ms       2.920us          3451  \r\n                                                                                                                        ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1         0.00%       0.000us         0.00%       0.000us       0.000us     776.446ms        49.42%     776.446ms     224.992us          3451  \r\nvoid at::native::(anonymous namespace)::CatArrayBatchedCopy<float, unsigned int, 4, 128, 1>(float*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<float, unsigned ...         0.00%       0.000us         0.00%       0.000us       0.000us      70.560ms         4.49%      70.560ms      86.897us           812  \r\n                                                                                               void cask_cudnn::computeOffsetsKernel<false, false>(cask_cudnn::ComputeOffsetsParams)         0.00%       0.000us         0.00%       0.000us       0.000us       2.670ms         0.17%       2.670ms       2.924us           913  \r\n                                                                                                                                            ampere_scudnn_128x64_relu_interior_nn_v1         0.00%       0.000us         0.00%       0.000us       0.000us      72.567ms         4.62%      72.567ms      89.368us           812  \r\nvoid at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::AddFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::...         0.00%       0.000us         0.00%       0.000us       0.000us     114.594ms         7.29%     114.594ms     125.376us           914  \r\nvoid at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl<at::native::(anonymous namespace)::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operat...         0.00%       0.000us         0.00%       0.000us       0.000us       7.095ms         0.45%       7.095ms      34.951us           203  \r\n                                                                                                                                              ampere_scudnn_128x64_relu_medium_nn_v1         0.00%       0.000us         0.00%       0.000us       0.000us       5.932ms         0.38%       5.932ms      58.733us           101  \r\n                                                                                                                                                               cudaDeviceSynchronize         7.29%      93.842ms         7.29%      93.842ms      93.842ms       0.000us         0.00%       0.000us       0.000us             1  \r\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n```\r\n\r\n### onnxruntime\r\nThis is profiled by turning on `enable_profiling` option and using modified [summarizing script](https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/transformers/profiler.py) so that it can skip first few run.\r\n\r\n#### Profile Result\r\n```\r\nGrouped by operator\r\n----------------------------------------------------------------\r\nTotal(μs)\tTime%\tKernel(μs)\tKernel%\tCalls\tAvgKernel(μs)\tFence(μs)\tOperator\r\n    991003\t91.46\t     990995\t91.46\t 3569\t         277.7\t         8\tConv\r\n     47059\t 4.34\t      47059\t 4.34\t 1992\t          23.6\t         0\tSplit\r\n     19171\t 1.77\t      19171\t 1.77\t 1992\t           9.6\t         0\tLeakyRelu\r\n     16982\t 1.57\t      16978\t 1.57\t  664\t          25.6\t         4\tConcat\r\n      8001\t 0.74\t       8001\t 0.74\t  747\t          10.7\t         0\tAdd\r\n      1343\t 0.12\t       1343\t 0.12\t   83\t          16.2\t         0\tDepthToSpace\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1108512661/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]