[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1134141849",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/11576#issuecomment-1134141849",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/11576",
        "id": 1134141849,
        "node_id": "IC_kwDOCVq1mM5DmaGZ",
        "user": {
            "login": "yeliang2258",
            "id": 30516196,
            "node_id": "MDQ6VXNlcjMwNTE2MTk2",
            "avatar_url": "https://avatars.githubusercontent.com/u/30516196?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yeliang2258",
            "html_url": "https://github.com/yeliang2258",
            "followers_url": "https://api.github.com/users/yeliang2258/followers",
            "following_url": "https://api.github.com/users/yeliang2258/following{/other_user}",
            "gists_url": "https://api.github.com/users/yeliang2258/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yeliang2258/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yeliang2258/subscriptions",
            "organizations_url": "https://api.github.com/users/yeliang2258/orgs",
            "repos_url": "https://api.github.com/users/yeliang2258/repos",
            "events_url": "https://api.github.com/users/yeliang2258/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yeliang2258/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-23T03:46:10Z",
        "updated_at": "2022-05-23T03:46:10Z",
        "author_association": "NONE",
        "body": "I found that almost all quantized models have this phenomenon. Turning off the optimization can align with the float model. Turning on the optimization leads to a drop in accuracy.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1134141849/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1134147175",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/11576#issuecomment-1134147175",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/11576",
        "id": 1134147175,
        "node_id": "IC_kwDOCVq1mM5DmbZn",
        "user": {
            "login": "yufenglee",
            "id": 30486710,
            "node_id": "MDQ6VXNlcjMwNDg2NzEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/30486710?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yufenglee",
            "html_url": "https://github.com/yufenglee",
            "followers_url": "https://api.github.com/users/yufenglee/followers",
            "following_url": "https://api.github.com/users/yufenglee/following{/other_user}",
            "gists_url": "https://api.github.com/users/yufenglee/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yufenglee/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yufenglee/subscriptions",
            "organizations_url": "https://api.github.com/users/yufenglee/orgs",
            "repos_url": "https://api.github.com/users/yufenglee/repos",
            "events_url": "https://api.github.com/users/yufenglee/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yufenglee/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-23T03:58:43Z",
        "updated_at": "2022-05-23T03:58:43Z",
        "author_association": "MEMBER",
        "body": "What kind of CPU do you run the model on? Could you please try quantizing the model with u8u8 format(both activation and weight uint8) https://onnxruntime.ai/docs/performance/quantization.html#when-and-why-do-i-need-to-try-u8u8 ?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1134147175/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1134148551",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/11576#issuecomment-1134148551",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/11576",
        "id": 1134148551,
        "node_id": "IC_kwDOCVq1mM5DmbvH",
        "user": {
            "login": "yeliang2258",
            "id": 30516196,
            "node_id": "MDQ6VXNlcjMwNTE2MTk2",
            "avatar_url": "https://avatars.githubusercontent.com/u/30516196?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yeliang2258",
            "html_url": "https://github.com/yeliang2258",
            "followers_url": "https://api.github.com/users/yeliang2258/followers",
            "following_url": "https://api.github.com/users/yeliang2258/following{/other_user}",
            "gists_url": "https://api.github.com/users/yeliang2258/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yeliang2258/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yeliang2258/subscriptions",
            "organizations_url": "https://api.github.com/users/yeliang2258/orgs",
            "repos_url": "https://api.github.com/users/yeliang2258/repos",
            "events_url": "https://api.github.com/users/yeliang2258/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yeliang2258/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-23T04:01:48Z",
        "updated_at": "2022-05-23T04:01:48Z",
        "author_association": "NONE",
        "body": "my cpu type is：Intel(R) Xeon(R) Gold 6271C",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1134148551/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1134151218",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/11576#issuecomment-1134151218",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/11576",
        "id": 1134151218,
        "node_id": "IC_kwDOCVq1mM5DmcYy",
        "user": {
            "login": "yufenglee",
            "id": 30486710,
            "node_id": "MDQ6VXNlcjMwNDg2NzEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/30486710?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yufenglee",
            "html_url": "https://github.com/yufenglee",
            "followers_url": "https://api.github.com/users/yufenglee/followers",
            "following_url": "https://api.github.com/users/yufenglee/following{/other_user}",
            "gists_url": "https://api.github.com/users/yufenglee/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yufenglee/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yufenglee/subscriptions",
            "organizations_url": "https://api.github.com/users/yufenglee/orgs",
            "repos_url": "https://api.github.com/users/yufenglee/repos",
            "events_url": "https://api.github.com/users/yufenglee/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yufenglee/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-23T04:06:10Z",
        "updated_at": "2022-05-23T04:06:10Z",
        "author_association": "MEMBER",
        "body": "> my cpu type is：Intel(R) Xeon(R) Gold 6271C\r\n\r\nIt doesn't have VNNI instructions. How do you generate the model? With ORT quantization tool or tf2onnx? If you are using ORT quantization tool, could you please try quantizing the model with u8u8 format and see if the performance gets better?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1134151218/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1134329674",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/11576#issuecomment-1134329674",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/11576",
        "id": 1134329674,
        "node_id": "IC_kwDOCVq1mM5DnH9K",
        "user": {
            "login": "yeliang2258",
            "id": 30516196,
            "node_id": "MDQ6VXNlcjMwNTE2MTk2",
            "avatar_url": "https://avatars.githubusercontent.com/u/30516196?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yeliang2258",
            "html_url": "https://github.com/yeliang2258",
            "followers_url": "https://api.github.com/users/yeliang2258/followers",
            "following_url": "https://api.github.com/users/yeliang2258/following{/other_user}",
            "gists_url": "https://api.github.com/users/yeliang2258/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yeliang2258/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yeliang2258/subscriptions",
            "organizations_url": "https://api.github.com/users/yeliang2258/orgs",
            "repos_url": "https://api.github.com/users/yeliang2258/repos",
            "events_url": "https://api.github.com/users/yeliang2258/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yeliang2258/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-23T08:09:53Z",
        "updated_at": "2022-05-23T08:45:22Z",
        "author_association": "NONE",
        "body": "> > my cpu type is：Intel(R) Xeon(R) Gold 6271C\r\n> \r\n> It doesn't have VNNI instructions. How do you generate the model? With ORT quantization tool or tf2onnx? If you are using ORT quantization tool, could you please try quantizing the model with u8u8 format and see if the performance gets better?\r\n\r\nThank you for your reply. \r\nI ran it again and found that there is indeed no problem of precision drop on the VNNI machine. Also, may I ask, symmetric quantization can be converted to a u8u8 format ONNX quantize model？",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1134329674/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1138898151",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/11576#issuecomment-1138898151",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/11576",
        "id": 1138898151,
        "node_id": "IC_kwDOCVq1mM5D4jTn",
        "user": {
            "login": "yufenglee",
            "id": 30486710,
            "node_id": "MDQ6VXNlcjMwNDg2NzEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/30486710?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yufenglee",
            "html_url": "https://github.com/yufenglee",
            "followers_url": "https://api.github.com/users/yufenglee/followers",
            "following_url": "https://api.github.com/users/yufenglee/following{/other_user}",
            "gists_url": "https://api.github.com/users/yufenglee/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yufenglee/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yufenglee/subscriptions",
            "organizations_url": "https://api.github.com/users/yufenglee/orgs",
            "repos_url": "https://api.github.com/users/yufenglee/repos",
            "events_url": "https://api.github.com/users/yufenglee/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yufenglee/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-26T18:44:01Z",
        "updated_at": "2022-05-26T18:44:01Z",
        "author_association": "MEMBER",
        "body": "> > > my cpu type is：Intel(R) Xeon(R) Gold 6271C\r\n> > \r\n> > \r\n> > It doesn't have VNNI instructions. How do you generate the model? With ORT quantization tool or tf2onnx? If you are using ORT quantization tool, could you please try quantizing the model with u8u8 format and see if the performance gets better?\r\n> \r\n> Thank you for your reply. I ran it again and found that there is indeed no problem of precision drop on the VNNI machine. Also, may I ask, symmetric quantization can be converted to a u8u8 format ONNX quantize model？\r\n\r\nThaks for your confirmation! So, you convert quantized model from TFLite. Yes, it can be converted to u8u8. You can do it by replacing int8 zeropoint in Q/DQ with uint8 by adding 128, and similar process to weight of Conv and Gemm/MatMul. \r\n\r\nWe will add an option to run a s8s8 model with u8u8 kernels on x64 natively in ORT for this kind of case. ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1138898151/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]