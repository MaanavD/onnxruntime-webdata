[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1158872531",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/11852#issuecomment-1158872531",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/11852",
        "id": 1158872531,
        "node_id": "IC_kwDOCVq1mM5FEv3T",
        "user": {
            "login": "CarlPoirier",
            "id": 5577772,
            "node_id": "MDQ6VXNlcjU1Nzc3NzI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5577772?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/CarlPoirier",
            "html_url": "https://github.com/CarlPoirier",
            "followers_url": "https://api.github.com/users/CarlPoirier/followers",
            "following_url": "https://api.github.com/users/CarlPoirier/following{/other_user}",
            "gists_url": "https://api.github.com/users/CarlPoirier/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/CarlPoirier/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/CarlPoirier/subscriptions",
            "organizations_url": "https://api.github.com/users/CarlPoirier/orgs",
            "repos_url": "https://api.github.com/users/CarlPoirier/repos",
            "events_url": "https://api.github.com/users/CarlPoirier/events{/privacy}",
            "received_events_url": "https://api.github.com/users/CarlPoirier/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-06-17T13:28:42Z",
        "updated_at": "2022-06-17T13:29:19Z",
        "author_association": "CONTRIBUTOR",
        "body": "Hi,\r\n\r\nI guess you are talking about the C or C++ API. If your ONNX model supports batch size execution, then you just have to use a properly shaped tensor. For example :\r\n```\r\nint64_t input_tensor[4] = {4 /*batch size*/, 3 /*channels*/, 224 /*height*/, 224 /*width*/}\r\nOrt::Value input_tensor = Ort::Value::CreateTensor(memory_info, dataptr, data_size, input_tensor, _countof(input_tensor));\r\n```\r\n\r\nFor a complete example, see [here](https://github.com/leimao/ONNX-Runtime-Inference/blob/main/src/inference.cpp).",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1158872531/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]