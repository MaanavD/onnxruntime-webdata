[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/573943962",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/2796#issuecomment-573943962",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/2796",
        "id": 573943962,
        "node_id": "MDEyOklzc3VlQ29tbWVudDU3Mzk0Mzk2Mg==",
        "user": {
            "login": "yufenglee",
            "id": 30486710,
            "node_id": "MDQ6VXNlcjMwNDg2NzEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/30486710?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yufenglee",
            "html_url": "https://github.com/yufenglee",
            "followers_url": "https://api.github.com/users/yufenglee/followers",
            "following_url": "https://api.github.com/users/yufenglee/following{/other_user}",
            "gists_url": "https://api.github.com/users/yufenglee/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yufenglee/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yufenglee/subscriptions",
            "organizations_url": "https://api.github.com/users/yufenglee/orgs",
            "repos_url": "https://api.github.com/users/yufenglee/repos",
            "events_url": "https://api.github.com/users/yufenglee/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yufenglee/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-01-14T00:43:54Z",
        "updated_at": "2020-01-14T00:45:06Z",
        "author_association": "MEMBER",
        "body": "@rameshjesswani , could you also try export the model with dynamic length, like I mentioned in the other thread:\r\n\r\n```\r\n    dynamic_axes = {\r\n        'input_id': {0:'batch',1:'max_seq_len'},\r\n        'sequence_id': {0:'batch',1:'max_seq_len'},\r\n        'input_mask': {0:'batch',1:'max_seq_len'},\r\n        'qp_scores': {0:'batch'},\r\n    }\r\n    torch.onnx.export(model, (input_ids, segment_ids, input_mask), config[\"onnx_model\"], verbose=False,\r\n        opset_version=11, input_names=['input_id', 'sequence_id', 'input_mask'],\r\n        output_names=['qp_scores'],\r\n        do_constant_folding=True, dynamic_axes=dynamic_axes)\r\n\r\n```\r\n\r\n#2803 \r\n\r\nAnd could you share us your model? We can take a look why the optimization doesn't take effect.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/573943962/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/574119139",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/2796#issuecomment-574119139",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/2796",
        "id": 574119139,
        "node_id": "MDEyOklzc3VlQ29tbWVudDU3NDExOTEzOQ==",
        "user": {
            "login": "rameshjes",
            "id": 25647309,
            "node_id": "MDQ6VXNlcjI1NjQ3MzA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/25647309?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rameshjes",
            "html_url": "https://github.com/rameshjes",
            "followers_url": "https://api.github.com/users/rameshjes/followers",
            "following_url": "https://api.github.com/users/rameshjes/following{/other_user}",
            "gists_url": "https://api.github.com/users/rameshjes/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rameshjes/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rameshjes/subscriptions",
            "organizations_url": "https://api.github.com/users/rameshjes/orgs",
            "repos_url": "https://api.github.com/users/rameshjes/repos",
            "events_url": "https://api.github.com/users/rameshjes/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rameshjes/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-01-14T10:57:24Z",
        "updated_at": "2020-01-14T10:57:24Z",
        "author_association": "NONE",
        "body": "thanks @yufenglee . I tried exporting with dynamic length. Now, inference of ONNX is better than Pytorch. \r\nSo here is the comparison after exporting with dynamic length:\r\n\r\nInference time of **Onnx** on 872 examples: `141.43` seconds\r\nInference time of **Pytorch** on 872 examples: `176.8` seconds\r\n\r\nJust another question, do you expect more improvement in onnx inference time as compare to pytorch? \r\n\r\nmany thanks :) ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/574119139/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/574289547",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/2796#issuecomment-574289547",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/2796",
        "id": 574289547,
        "node_id": "MDEyOklzc3VlQ29tbWVudDU3NDI4OTU0Nw==",
        "user": {
            "login": "yufenglee",
            "id": 30486710,
            "node_id": "MDQ6VXNlcjMwNDg2NzEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/30486710?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yufenglee",
            "html_url": "https://github.com/yufenglee",
            "followers_url": "https://api.github.com/users/yufenglee/followers",
            "following_url": "https://api.github.com/users/yufenglee/following{/other_user}",
            "gists_url": "https://api.github.com/users/yufenglee/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yufenglee/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yufenglee/subscriptions",
            "organizations_url": "https://api.github.com/users/yufenglee/orgs",
            "repos_url": "https://api.github.com/users/yufenglee/repos",
            "events_url": "https://api.github.com/users/yufenglee/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yufenglee/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-01-14T17:38:23Z",
        "updated_at": "2020-01-14T17:38:39Z",
        "author_association": "MEMBER",
        "body": "You can get better performance with GraphOptimization by replacing code:\r\n```\r\n    #onnx session\r\n    ort_session = onnxruntime.InferenceSession(model_name)\r\n```\r\nwith\r\n\r\n```\r\nso = ort.SessionOptions()\r\nso.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\r\nort_session = onnxruntime.InferenceSession(model_name, so)\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/574289547/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/574405330",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/2796#issuecomment-574405330",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/2796",
        "id": 574405330,
        "node_id": "MDEyOklzc3VlQ29tbWVudDU3NDQwNTMzMA==",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-01-14T22:31:48Z",
        "updated_at": "2020-01-14T22:31:48Z",
        "author_association": "MEMBER",
        "body": "To verify BERT optimization, add a session option to output the optimized model like\r\n```\r\nso = ort.SessionOptions()\r\nso.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\r\nso.optimized_model_filepath = \"optimized.onnx\"\r\nort_session = onnxruntime.InferenceSession(model_name, so)\r\n```\r\nAfter running, optimized.onnx can be found. Then you can open the optimized.onnx using [Netron](https://www.lutzroeder.com/ai/) to view the graph. The optimized graph for CPU is like the following:\r\n```\r\n input_ids  segment_ids  input_mask\r\n    |            |           |\r\n   Cast         Cast        Cast\r\n    \\            |          /\r\n      EmbedLayerNormalization\r\n                 |        |\r\n               Attention  |                    --- start of one layer\r\n                 |        |\r\n               MatMul    /\r\n                 |      /\r\n                Add    /\r\n                  \\   /   \r\n                   Add\r\n                    |\r\n              LayerNormalization\r\n                    |        |\r\n                 MatMul      |\r\n                    |        |\r\n                 BaisGelu    |\r\n                    |        |\r\n                  MatMul    /\r\n                    |      /\r\n                   Add    /\r\n                     \\   /\r\n                      Add\r\n                       |\r\n            LayerNormalization                  ---  end of one layer\r\n                 |        |\r\n               Attention  |                     --- start of next layer (total 12 layers for BERT base model)\r\n                 |        |\r\n                    ...\r\n```\r\nIf you run onnxruntime in GPU, the optimized graph is slightly different.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/574405330/reactions",
            "total_count": 2,
            "+1": 2,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/574603101",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/2796#issuecomment-574603101",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/2796",
        "id": 574603101,
        "node_id": "MDEyOklzc3VlQ29tbWVudDU3NDYwMzEwMQ==",
        "user": {
            "login": "rameshjes",
            "id": 25647309,
            "node_id": "MDQ6VXNlcjI1NjQ3MzA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/25647309?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rameshjes",
            "html_url": "https://github.com/rameshjes",
            "followers_url": "https://api.github.com/users/rameshjes/followers",
            "following_url": "https://api.github.com/users/rameshjes/following{/other_user}",
            "gists_url": "https://api.github.com/users/rameshjes/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rameshjes/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rameshjes/subscriptions",
            "organizations_url": "https://api.github.com/users/rameshjes/orgs",
            "repos_url": "https://api.github.com/users/rameshjes/repos",
            "events_url": "https://api.github.com/users/rameshjes/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rameshjes/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-01-15T10:43:38Z",
        "updated_at": "2020-01-15T10:43:38Z",
        "author_association": "NONE",
        "body": "> You can get better performance with GraphOptimization by replacing code:\r\n> \r\n> ```\r\n>     #onnx session\r\n>     ort_session = onnxruntime.InferenceSession(model_name)\r\n> ```\r\n> \r\n> with\r\n> \r\n> ```\r\n> so = ort.SessionOptions()\r\n> so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\r\n> ort_session = onnxruntime.InferenceSession(model_name, so)\r\n> ```\r\n\r\nI am using the same parameters, that you have specified. Here is way of exporting pytorch model to onnx: \r\n```\r\ndef convert_bert_to_onnx(text, model_dir, task_name, onnx_model_name):\r\n\r\n        config = BertConfig.from_pretrained(model_dir)\r\n        tokenizer = BertTokenizer.from_pretrained(model_dir)\r\n        model = BertForSequenceClassification.from_pretrained(model_dir, config=config)\r\n        model.to(\"cpu\")\r\n        input_ids, input_mask, segment_ids = preprocess(tokenizer, text)\r\n       \r\n        dynamic_axes = {\r\n            'input_id': {0: 1, 1:128},\r\n            'input_mask': {0:1, 1:128},\r\n            'segment_ids': {0: 1 ,1:128},\r\n            'output': {0: 1},\r\n        }\r\n\r\n        torch.onnx.export(model, (input_ids, input_mask, segment_ids), onnx_model_name,  input_names = [\"input_ids\", \"input_mask\", \"segment_ids\"],\r\n        output_names = [\"output\"], opset_version=10, do_constant_folding=True)\r\n\r\n        print(\"SST model convert to onnx format successfully\")\r\n\r\n```\r\n\r\nHere is how I am performing inference: \r\n\r\n```\r\n#onnx session\r\nso = onnxruntime.SessionOptions()\r\nso.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL\r\nso.intra_op_num_threads=1\r\nort_session = onnxruntime.InferenceSession(onnx_model_name, so)\r\n\r\nfor example in examples:\r\n        input_ids, input_mask, segment_ids = preprocess(tokenizer, example)\r\n\r\n\r\n       ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(input_ids),\r\n                        ort_session.get_inputs()[1].name: to_numpy(input_mask),\r\n                        ort_session.get_inputs()[2].name: to_numpy(segment_ids)}\r\n        ort_outs = ort_session.run([\"output\"], ort_inputs)\r\n        torch_onnx_output = torch.tensor(ort_outs[0], dtype=torch.float32)\r\n        onnx_logits = F.softmax(torch_onnx_output, dim=1)\r\n        \r\n        logits_label = torch.argmax(onnx_logits, dim=1)\r\n        label = logits_label.detach().cpu().numpy()\r\n        onnx_inference.append(label[0])\r\n\r\n```\r\n\r\nThanks :) ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/574603101/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/574931275",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/2796#issuecomment-574931275",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/2796",
        "id": 574931275,
        "node_id": "MDEyOklzc3VlQ29tbWVudDU3NDkzMTI3NQ==",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-01-16T01:04:48Z",
        "updated_at": "2020-01-16T01:11:04Z",
        "author_association": "MEMBER",
        "body": "@rameshjesswani, have you checked whether the graph of optimized model is same as I described above?\r\n\r\nIn my machine, inference time of onnxruntime is 47% of that of PyTorch 1.3 when running BERT base (batch_size=8, max_seq_len=128) on SQuAD dev set. Different result might be seen in your hardware.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/574931275/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/575065822",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/2796#issuecomment-575065822",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/2796",
        "id": 575065822,
        "node_id": "MDEyOklzc3VlQ29tbWVudDU3NTA2NTgyMg==",
        "user": {
            "login": "rameshjes",
            "id": 25647309,
            "node_id": "MDQ6VXNlcjI1NjQ3MzA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/25647309?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rameshjes",
            "html_url": "https://github.com/rameshjes",
            "followers_url": "https://api.github.com/users/rameshjes/followers",
            "following_url": "https://api.github.com/users/rameshjes/following{/other_user}",
            "gists_url": "https://api.github.com/users/rameshjes/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rameshjes/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rameshjes/subscriptions",
            "organizations_url": "https://api.github.com/users/rameshjes/orgs",
            "repos_url": "https://api.github.com/users/rameshjes/repos",
            "events_url": "https://api.github.com/users/rameshjes/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rameshjes/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-01-16T09:37:00Z",
        "updated_at": "2020-01-16T09:40:21Z",
        "author_association": "NONE",
        "body": "@tianleiwu its not same, it seems different. I am using BERT large and pyTorch 1.2 . Could you tell me where to share this model with you, so you can test on your hardware. Thank you :) \r\n\r\nAlso, does batch size affects the inference time much in onnxruntime? I am comparing pyTorch and onnxruntime using `batch size = 1`  \r\n\r\n![ss](https://user-images.githubusercontent.com/25647309/72511180-fe18e080-384a-11ea-9a8b-65a964c1c184.png)\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/575065822/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/575820648",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/2796#issuecomment-575820648",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/2796",
        "id": 575820648,
        "node_id": "MDEyOklzc3VlQ29tbWVudDU3NTgyMDY0OA==",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-01-17T22:31:27Z",
        "updated_at": "2020-01-17T22:31:27Z",
        "author_association": "MEMBER",
        "body": "@rameshjesswani, I've tried PyTorch1.2 to export a Bert large model for SQuAD based on [transformers](https://github.com/huggingface/transformers/blob/master/examples/run_squad.py). The graph is like the one you posted.\r\n\r\nYou can add a session option to output the optimized model and open the optimized.onnx file to view the optimized graph:\r\n```\r\nso.optimized_model_filepath = \"optimized.onnx\"\r\n```\r\n\r\nThe optimized graph is like the following:\r\n![image](https://user-images.githubusercontent.com/30328909/72650210-a191f000-3934-11ea-9ac0-b3d3d31a3272.png)\r\n\r\nIn my machine, inference time of onnxruntime is 73% of that of PyTorch 1.2 when running BERT large (batch_size=1, max_seq_len=128) on SQuAD dev set. In your experiment, the ratio is 80%. So our results are very close.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/575820648/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/576164926",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/2796#issuecomment-576164926",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/2796",
        "id": 576164926,
        "node_id": "MDEyOklzc3VlQ29tbWVudDU3NjE2NDkyNg==",
        "user": {
            "login": "rameshjes",
            "id": 25647309,
            "node_id": "MDQ6VXNlcjI1NjQ3MzA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/25647309?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rameshjes",
            "html_url": "https://github.com/rameshjes",
            "followers_url": "https://api.github.com/users/rameshjes/followers",
            "following_url": "https://api.github.com/users/rameshjes/following{/other_user}",
            "gists_url": "https://api.github.com/users/rameshjes/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rameshjes/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rameshjes/subscriptions",
            "organizations_url": "https://api.github.com/users/rameshjes/orgs",
            "repos_url": "https://api.github.com/users/rameshjes/repos",
            "events_url": "https://api.github.com/users/rameshjes/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rameshjes/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-01-20T08:34:19Z",
        "updated_at": "2020-01-20T08:34:19Z",
        "author_association": "NONE",
        "body": "@tianleiwu thanks. Just to summarize this, using BERT Base, you observed inference time of onnxruntime is `47%` of that of `PyTorch 1.3` and using BERT Large, inference time of onnxruntime is `73% ` of PyTorch. \r\n\r\nThere is huge difference between improvement in inference time of BERT Base and Large as compare to the PyTorch, is this the expected behavior? \r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/576164926/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/576955337",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/2796#issuecomment-576955337",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/2796",
        "id": 576955337,
        "node_id": "MDEyOklzc3VlQ29tbWVudDU3Njk1NTMzNw==",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-01-22T00:43:59Z",
        "updated_at": "2020-01-22T00:43:59Z",
        "author_association": "MEMBER",
        "body": "@rameshjesswani, this is expected since model size is different. Note that inference time improvement could differ for different models on different hardware.\r\n\r\nFor BERT-large, it is not likely using CPU in production due to the latency. It need powerful GPU (like V100 or T4) to achieve reasonable latency (<5 ms) for real time inference.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/576955337/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/577060733",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/2796#issuecomment-577060733",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/2796",
        "id": 577060733,
        "node_id": "MDEyOklzc3VlQ29tbWVudDU3NzA2MDczMw==",
        "user": {
            "login": "rameshjes",
            "id": 25647309,
            "node_id": "MDQ6VXNlcjI1NjQ3MzA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/25647309?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rameshjes",
            "html_url": "https://github.com/rameshjes",
            "followers_url": "https://api.github.com/users/rameshjes/followers",
            "following_url": "https://api.github.com/users/rameshjes/following{/other_user}",
            "gists_url": "https://api.github.com/users/rameshjes/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rameshjes/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rameshjes/subscriptions",
            "organizations_url": "https://api.github.com/users/rameshjes/orgs",
            "repos_url": "https://api.github.com/users/rameshjes/repos",
            "events_url": "https://api.github.com/users/rameshjes/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rameshjes/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-01-22T08:19:15Z",
        "updated_at": "2020-01-22T08:19:15Z",
        "author_association": "NONE",
        "body": "@tianleiwu many thanks. I am closing this issue as i have got the answer :) ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/577060733/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/729507809",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/2796#issuecomment-729507809",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/2796",
        "id": 729507809,
        "node_id": "MDEyOklzc3VlQ29tbWVudDcyOTUwNzgwOQ==",
        "user": {
            "login": "chazo1994",
            "id": 17020334,
            "node_id": "MDQ6VXNlcjE3MDIwMzM0",
            "avatar_url": "https://avatars.githubusercontent.com/u/17020334?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/chazo1994",
            "html_url": "https://github.com/chazo1994",
            "followers_url": "https://api.github.com/users/chazo1994/followers",
            "following_url": "https://api.github.com/users/chazo1994/following{/other_user}",
            "gists_url": "https://api.github.com/users/chazo1994/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/chazo1994/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/chazo1994/subscriptions",
            "organizations_url": "https://api.github.com/users/chazo1994/orgs",
            "repos_url": "https://api.github.com/users/chazo1994/repos",
            "events_url": "https://api.github.com/users/chazo1994/events{/privacy}",
            "received_events_url": "https://api.github.com/users/chazo1994/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-11-18T08:03:43Z",
        "updated_at": "2020-11-18T08:03:43Z",
        "author_association": "NONE",
        "body": "> @tianleiwu thanks. Just to summarize this, using BERT Base, you observed inference time of onnxruntime is `47%` of that of `PyTorch 1.3` and using BERT Large, inference time of onnxruntime is `73% ` of PyTorch.\r\n\r\nCan you share which device did you used?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/729507809/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]