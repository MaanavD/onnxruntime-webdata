[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/674601621",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/4816#issuecomment-674601621",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4816",
        "id": 674601621,
        "node_id": "MDEyOklzc3VlQ29tbWVudDY3NDYwMTYyMQ==",
        "user": {
            "login": "hariharans29",
            "id": 9969784,
            "node_id": "MDQ6VXNlcjk5Njk3ODQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9969784?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hariharans29",
            "html_url": "https://github.com/hariharans29",
            "followers_url": "https://api.github.com/users/hariharans29/followers",
            "following_url": "https://api.github.com/users/hariharans29/following{/other_user}",
            "gists_url": "https://api.github.com/users/hariharans29/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hariharans29/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hariharans29/subscriptions",
            "organizations_url": "https://api.github.com/users/hariharans29/orgs",
            "repos_url": "https://api.github.com/users/hariharans29/repos",
            "events_url": "https://api.github.com/users/hariharans29/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hariharans29/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-08-17T00:38:06Z",
        "updated_at": "2020-08-17T00:38:06Z",
        "author_association": "MEMBER",
        "body": "Not sure if ORT_ENABLE_ALL ***guarantees*** perf gain when compared to ORT_DISABLE_ALL. ORT_ENABLE_ALL tries to enable optimizations in the model if applicable. But, if the model is already optimized and there is nothing left for ORT_ENABLE_ALL, then it is expected to have no diff between enabling all optimizations and disabling all optimizations isn't it ?\r\n\r\nThe correct comparison should be between the base framework perf and ORT perf after ORT_ENABLE_ALL. If you see a diff here, that warrants investigation. I don't think not seeing a diff between ORT_DISABLE_ALL and ORT_ENABLE_ALL is an issue (it may happen sometimes by the above logic)  ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/674601621/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/675005510",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/4816#issuecomment-675005510",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4816",
        "id": 675005510,
        "node_id": "MDEyOklzc3VlQ29tbWVudDY3NTAwNTUxMA==",
        "user": {
            "login": "tracysh",
            "id": 42477615,
            "node_id": "MDQ6VXNlcjQyNDc3NjE1",
            "avatar_url": "https://avatars.githubusercontent.com/u/42477615?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tracysh",
            "html_url": "https://github.com/tracysh",
            "followers_url": "https://api.github.com/users/tracysh/followers",
            "following_url": "https://api.github.com/users/tracysh/following{/other_user}",
            "gists_url": "https://api.github.com/users/tracysh/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tracysh/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tracysh/subscriptions",
            "organizations_url": "https://api.github.com/users/tracysh/orgs",
            "repos_url": "https://api.github.com/users/tracysh/repos",
            "events_url": "https://api.github.com/users/tracysh/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tracysh/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-08-17T17:17:47Z",
        "updated_at": "2020-08-17T17:17:47Z",
        "author_association": "CONTRIBUTOR",
        "body": "Hi, I took a look at the model and have a few comments:\r\n\r\n1. The different optimizations levels aren't having an impact because the model appears to have been exported with keep_initializers_as_inputs=True through torch.onnx.export. This signals to the runtime that any of the weights may be overriden at inference time, so none of the optimizations apply. I see this debug message when using the model with onnxruntime_perf_test:\r\n\r\n2020-08-17 08:49:14.6667507 [W:onnxruntime:, graph.cc:863 onnxruntime::Graph::Graph] Initializer 90 appears in graph inp\r\nuts and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const foldi\r\nng. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest export\r\ner/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\r\n\r\n2. I edited the model to remove the weights from the inputs list. This then allowed various optimizations such as Conv+BatchNorm fusion and NCHWc layout transformation to occur. However, the overall performance ended up much worse. I'm seeing that the weights contain denormal numbers which results in a very slow path in the processor (an order of magnitude worse for a multiply/add instruction).\r\n\r\nI'm curious if you are also seeing a slowdown when doing an inference with PyTorch. PyTorch forums point users at torch.set_flush_denormal(true) to enable a CPU mode that improves performance by returning zero instead of attempting to compute the correct denormal. ONNX Runtime does not have an equivalent for that today. I modified the runtime to force this mode on all the threads and then performance improved greatly: ~400ms->150ms. I see the same behavior using the Intel DNNL execution provider.\r\n\r\nIf the weights could be post-processed to clip very small numbers, that should avoid the denormal problem. I've just started seeing more models with this problem recently: I don't have a plan at this time of how the runtime might have an equivalent to set_flush_denormal (or go the TensorFlow and always flush denormals to zero). ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/675005510/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/675297256",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/4816#issuecomment-675297256",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4816",
        "id": 675297256,
        "node_id": "MDEyOklzc3VlQ29tbWVudDY3NTI5NzI1Ng==",
        "user": {
            "login": "sapjunior",
            "id": 987946,
            "node_id": "MDQ6VXNlcjk4Nzk0Ng==",
            "avatar_url": "https://avatars.githubusercontent.com/u/987946?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/sapjunior",
            "html_url": "https://github.com/sapjunior",
            "followers_url": "https://api.github.com/users/sapjunior/followers",
            "following_url": "https://api.github.com/users/sapjunior/following{/other_user}",
            "gists_url": "https://api.github.com/users/sapjunior/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/sapjunior/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/sapjunior/subscriptions",
            "organizations_url": "https://api.github.com/users/sapjunior/orgs",
            "repos_url": "https://api.github.com/users/sapjunior/repos",
            "events_url": "https://api.github.com/users/sapjunior/events{/privacy}",
            "received_events_url": "https://api.github.com/users/sapjunior/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-08-18T07:05:10Z",
        "updated_at": "2020-08-18T07:05:10Z",
        "author_association": "NONE",
        "body": "@tracysh Thank you for very clear clarification. After cliping and replacing small weights with zero, the inference speed is significantly improve, in my case 3 times better than original model.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/675297256/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]