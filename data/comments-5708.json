[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/722220666",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/5708#issuecomment-722220666",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/5708",
        "id": 722220666,
        "node_id": "MDEyOklzc3VlQ29tbWVudDcyMjIyMDY2Ng==",
        "user": {
            "login": "yufenglee",
            "id": 30486710,
            "node_id": "MDQ6VXNlcjMwNDg2NzEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/30486710?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yufenglee",
            "html_url": "https://github.com/yufenglee",
            "followers_url": "https://api.github.com/users/yufenglee/followers",
            "following_url": "https://api.github.com/users/yufenglee/following{/other_user}",
            "gists_url": "https://api.github.com/users/yufenglee/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yufenglee/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yufenglee/subscriptions",
            "organizations_url": "https://api.github.com/users/yufenglee/orgs",
            "repos_url": "https://api.github.com/users/yufenglee/repos",
            "events_url": "https://api.github.com/users/yufenglee/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yufenglee/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-11-05T08:20:49Z",
        "updated_at": "2020-11-05T08:20:49Z",
        "author_association": "MEMBER",
        "body": "ORT is working on supporting quantization on GPU, i.e., your quantization model is running on CPU instead of GPU. Note that even after GPU quantization is implemented, you still need GPU with arch >= Turing to get better performance. GTX1050i is definitely too old and won't get benefit.\r\n\r\nBTW, as mentioned in the other thread, performance improvement with quantization needs hardware support, and is model dependent. On CPU with AVX2, AVX512 extension, you can get good performance improvement for a I/O bound model, but not compute bound model. You need CPU with VNNI (AVX512 VNNI or AVXVNNI) to get better performance for a compute bound model. ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/722220666/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/770639468",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/5708#issuecomment-770639468",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/5708",
        "id": 770639468,
        "node_id": "MDEyOklzc3VlQ29tbWVudDc3MDYzOTQ2OA==",
        "user": {
            "login": "jay-karan",
            "id": 65300616,
            "node_id": "MDQ6VXNlcjY1MzAwNjE2",
            "avatar_url": "https://avatars.githubusercontent.com/u/65300616?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jay-karan",
            "html_url": "https://github.com/jay-karan",
            "followers_url": "https://api.github.com/users/jay-karan/followers",
            "following_url": "https://api.github.com/users/jay-karan/following{/other_user}",
            "gists_url": "https://api.github.com/users/jay-karan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jay-karan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jay-karan/subscriptions",
            "organizations_url": "https://api.github.com/users/jay-karan/orgs",
            "repos_url": "https://api.github.com/users/jay-karan/repos",
            "events_url": "https://api.github.com/users/jay-karan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jay-karan/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-02-01T07:35:12Z",
        "updated_at": "2021-02-01T07:35:12Z",
        "author_association": "NONE",
        "body": "Hi @yufenglee, How do we differentiate I/O bound models and Compute models. Further more, I tried the similar thing on RTX 2080, quantized models were still performing slower compared to non-quantized models. How do I overcome this?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/770639468/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]