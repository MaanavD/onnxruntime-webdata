[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/756334256",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/6229#issuecomment-756334256",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/6229",
        "id": 756334256,
        "node_id": "MDEyOklzc3VlQ29tbWVudDc1NjMzNDI1Ng==",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-01-07T19:33:56Z",
        "updated_at": "2021-01-07T19:34:54Z",
        "author_association": "MEMBER",
        "body": "@carter54, \r\n\r\nI suggest to use convert_to_onnx to output the optimized fp16 GPT-2 model directly like\r\n```\r\npython convert_to_onnx.py -m gpt2 --model_class GPT2LMHeadModel --output gpt2.onnx -p fp16 -o --use_gpu\r\n```\r\n\r\nI tried benchmark in a V100 machine (I changed sequence_length=1 to sequence_length=200 in benchmark_gpt2.py in the master branch):\r\n```\r\npython benchmark_gpt2.py -m gpt2 --model_class GPT2LMHeadModel --test_times 100 -o --use_gpu -p fp16 -b 5 -s 0                      \r\n```\r\n\r\nThe output is like the following:\r\n```\r\nArguments:Namespace(batch_sizes=[5], cache_dir='./cache_models', include_copy_output_latency=False, model_class='GPT2LMHeadModel', model_name_or_path='gpt2', onnx_dir='./onnx_models', optimize_onnx=True, past_sequence_lengths=[0], precision=<Precision.FLOAT16: 'fp16'>, result_csv=None, test_times=100, thread_num=-1, torchscript=False, use_gpu=True, validate_onnx=False, verbose=False)\r\nATen/Parallel:\r\n        at::get_num_threads() : 24\r\n        at::get_num_interop_threads() : 12\r\nOpenMP 201511 (a.k.a. OpenMP 4.5)\r\n        omp_get_max_threads() : 24\r\nIntel(R) Math Kernel Library Version 2020.0.1 Product Build 20200208 for Intel(R) 64 architecture applications\r\n        mkl_get_max_threads() : 24\r\nIntel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)\r\nstd::thread::hardware_concurrency() : 24\r\nEnvironment variables:\r\n        OMP_NUM_THREADS : 16\r\n        MKL_NUM_THREADS : [not set]\r\nATen parallel backend: OpenMP\r\n\r\nPyTorch Version:1.6.0\r\nTransformers Version:3.1.0\r\nOnnxruntime Version:1.5.2\r\nShapes: input_ids=torch.Size([1, 1]) past=torch.Size([2, 1, 12, 1, 64]) output=torch.Size([1, 1, 50257]) present=torch.Size([2, 1, 12, 2, 64])\r\n/bert_ort/tlwu/py36/lib/python3.6/site-packages/transformers/modeling_gpt2.py:558: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  assert batch_size > 0, \"batch_size has to be defined and > 0\"\r\n/bert_ort/tlwu/py36/lib/python3.6/site-packages/transformers/modeling_gpt2.py:165: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  w = w / (float(v.size(-1)) ** 0.5)\r\n/bert_ort/tlwu/py36/lib/python3.6/site-packages/transformers/modeling_gpt2.py:170: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  mask = self.bias[:, :, ns - nd : ns, :ns]\r\nFused LayerNormalization count: 25\r\nFused FastGelu count: 12\r\nRemoved Reshape and Expand count: 0\r\nFused Attention(with past) count: 12\r\nGraph pruned: 0 inputs, 0 outputs and 741 nodes are removed\r\nGraph pruned: 0 inputs, 0 outputs and 312 nodes are removed\r\npostprocess: remove Reshape count:48\r\nFused FastGelu(add bias) count: 12\r\nopset verion: 11\r\nOutput model to ./onnx_models/gpt2_past_fp16.onnx\r\nbatch_size=5, past_sequence_length=0, torch_latency=64.92, ort_latency=92.17, ort_io_latency=17.29\r\n```\r\nIn my test, the latency with IO Binding is 17ms, and without IO Binding is 92ms.\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/756334256/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/756594158",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/6229#issuecomment-756594158",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/6229",
        "id": 756594158,
        "node_id": "MDEyOklzc3VlQ29tbWVudDc1NjU5NDE1OA==",
        "user": {
            "login": "carter54",
            "id": 26741594,
            "node_id": "MDQ6VXNlcjI2NzQxNTk0",
            "avatar_url": "https://avatars.githubusercontent.com/u/26741594?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/carter54",
            "html_url": "https://github.com/carter54",
            "followers_url": "https://api.github.com/users/carter54/followers",
            "following_url": "https://api.github.com/users/carter54/following{/other_user}",
            "gists_url": "https://api.github.com/users/carter54/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/carter54/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/carter54/subscriptions",
            "organizations_url": "https://api.github.com/users/carter54/orgs",
            "repos_url": "https://api.github.com/users/carter54/repos",
            "events_url": "https://api.github.com/users/carter54/events{/privacy}",
            "received_events_url": "https://api.github.com/users/carter54/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-01-08T07:16:35Z",
        "updated_at": "2021-01-08T07:16:35Z",
        "author_association": "NONE",
        "body": "@tianleiwu Thx for the reply. I can reproduce the similar result with yours now with this:\r\n```\r\npython benchmark_gpt2.py -m gpt2 --model_class GPT2LMHeadModel --test_times 100 -o --use_gpu -p fp16 -b 5 -s 0\r\n```\r\n\r\nBut following error appeared when I run\r\n```\r\npython benchmark_gpt2.py -m gpt2 --model_class GPT2LMHeadModel --test_times 100 -o --use_gpu -p fp16 -b 5 -s 200\r\n```\r\nafter changing '-s 0' to '-s 200'\r\n```\r\nArguments:Namespace(batch_sizes=[5], cache_dir='./cache_models', include_copy_output_latency=False, model_class='GPT2LMHeadModel', model_name_or_path='/home/hr/PycharmProjects/test/', onnx_dir='./onnx_models', optimize_onnx=True, past_sequence_lengths=[200], precision=<Precision.FLOAT16: 'fp16'>, result_csv=None, test_times=100, thread_num=-1, torchscript=False, use_gpu=True, validate_onnx=False, verbose=False)\r\nATen/Parallel:\r\n        at::get_num_threads() : 24\r\n        at::get_num_interop_threads() : 12\r\nOpenMP 201511 (a.k.a. OpenMP 4.5)\r\n        omp_get_max_threads() : 24\r\nIntel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\r\n        mkl_get_max_threads() : 24\r\nIntel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)\r\nstd::thread::hardware_concurrency() : 24\r\nEnvironment variables:\r\n        OMP_NUM_THREADS : [not set]\r\n        MKL_NUM_THREADS : [not set]\r\nATen parallel backend: OpenMP\r\n\r\nPyTorch Version:1.7.1\r\nTransformers Version:3.1.0\r\nOnnxruntime Version:1.5.2\r\n/home/hr/anaconda3/lib/python3.8/site-packages/transformers/modeling_gpt2.py:710: FutureWarning: The `past` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\r\n  warnings.warn(\r\nShapes: input_ids=torch.Size([1, 1]) past=torch.Size([2, 1, 12, 1, 64]) output=torch.Size([1, 1, 30000]) present=torch.Size([2, 1, 12, 2, 64])\r\n/home/hr/anaconda3/lib/python3.8/site-packages/transformers/modeling_gpt2.py:558: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  assert batch_size > 0, \"batch_size has to be defined and > 0\"\r\n/home/hr/anaconda3/lib/python3.8/site-packages/transformers/modeling_gpt2.py:165: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  w = w / (float(v.size(-1)) ** 0.5)\r\n/home/hr/anaconda3/lib/python3.8/site-packages/transformers/modeling_gpt2.py:170: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  mask = self.bias[:, :, ns - nd : ns, :ns]\r\nFused LayerNormalization count: 25\r\nFused FastGelu count: 12\r\nFused Attention(with past) count: 12\r\nGraph pruned: 0 inputs, 0 outputs and 741 nodes are removed\r\nGraph pruned: 0 inputs, 0 outputs and 312 nodes are removed\r\npostprocess: remove Reshape count:48\r\nFused FastGelu(add bias) count: 12\r\nopset verion: 11\r\nOutput model to ./onnx_models/model_past_fp16.onnx\r\nException\r\nTraceback (most recent call last):\r\n  File \"benchmark_gpt2.py\", line 214, in main\r\n    ort_io_outputs, ort_io_latency = Gpt2Helper.onnxruntime_inference_with_binded_io(\r\n  File \"/home/hr/anaconda3/lib/python3.8/site-packages/onnxruntime/transformers/gpt2_helper.py\", line 453, in onnxruntime_inference_with_binded_io\r\n    io_binding = Gpt2Helper.prepare_io_binding(ort_session, inputs.input_ids, inputs.position_ids,\r\n  File \"/home/hr/anaconda3/lib/python3.8/site-packages/onnxruntime/transformers/gpt2_helper.py\", line 410, in prepare_io_binding\r\n    assert position_ids.is_contiguous()\r\nAssertionError\r\n```\r\nif I remove line 410 in gpt2_helper.py\r\n```\r\nassert position_ids.is_contiguous()\r\n```\r\nthis script works fine. I can get following result:\r\n```\r\nArguments:Namespace(batch_sizes=[5], cache_dir='./cache_models', include_copy_output_latency=False, model_class='GPT2LMHeadModel', model_name_or_path='/home/hr/PycharmProjects/test/', onnx_dir='./onnx_models', optimize_onnx=True, past_sequence_lengths=[200], precision=<Precision.FLOAT16: 'fp16'>, result_csv=None, test_times=100, thread_num=-1, torchscript=False, use_gpu=True, validate_onnx=False, verbose=False)\r\nATen/Parallel:\r\n        at::get_num_threads() : 24\r\n        at::get_num_interop_threads() : 12\r\nOpenMP 201511 (a.k.a. OpenMP 4.5)\r\n        omp_get_max_threads() : 24\r\nIntel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\r\n        mkl_get_max_threads() : 24\r\nIntel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)\r\nstd::thread::hardware_concurrency() : 24\r\nEnvironment variables:\r\n        OMP_NUM_THREADS : [not set]\r\n        MKL_NUM_THREADS : [not set]\r\nATen parallel backend: OpenMP\r\n\r\nPyTorch Version:1.7.1\r\nTransformers Version:3.1.0\r\nOnnxruntime Version:1.5.2\r\n/home/hr/anaconda3/lib/python3.8/site-packages/transformers/modeling_gpt2.py:710: FutureWarning: The `past` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\r\n  warnings.warn(\r\nShapes: input_ids=torch.Size([1, 1]) past=torch.Size([2, 1, 12, 1, 64]) output=torch.Size([1, 1, 30000]) present=torch.Size([2, 1, 12, 2, 64])\r\n/home/hr/anaconda3/lib/python3.8/site-packages/transformers/modeling_gpt2.py:558: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  assert batch_size > 0, \"batch_size has to be defined and > 0\"\r\n/home/hr/anaconda3/lib/python3.8/site-packages/transformers/modeling_gpt2.py:165: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  w = w / (float(v.size(-1)) ** 0.5)\r\n/home/hr/anaconda3/lib/python3.8/site-packages/transformers/modeling_gpt2.py:170: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  mask = self.bias[:, :, ns - nd : ns, :ns]\r\nFused LayerNormalization count: 25\r\nFused FastGelu count: 12\r\nFused Attention(with past) count: 12\r\nGraph pruned: 0 inputs, 0 outputs and 741 nodes are removed\r\nGraph pruned: 0 inputs, 0 outputs and 312 nodes are removed\r\npostprocess: remove Reshape count:48\r\nFused FastGelu(add bias) count: 12\r\nopset verion: 11\r\nOutput model to ./onnx_models/test_past_fp16.onnx\r\nbatch_size=5, past_sequence_length=200, torch_latency=25.55, ort_latency=33.30, ort_io_latency=4.46\r\nResults are saved to file benchmark_result_20210108-150937.csv\r\n```\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/756594158/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]