[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/809849005",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/7144#issuecomment-809849005",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/7144",
        "id": 809849005,
        "node_id": "MDEyOklzc3VlQ29tbWVudDgwOTg0OTAwNQ==",
        "user": {
            "login": "yufenglee",
            "id": 30486710,
            "node_id": "MDQ6VXNlcjMwNDg2NzEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/30486710?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yufenglee",
            "html_url": "https://github.com/yufenglee",
            "followers_url": "https://api.github.com/users/yufenglee/followers",
            "following_url": "https://api.github.com/users/yufenglee/following{/other_user}",
            "gists_url": "https://api.github.com/users/yufenglee/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yufenglee/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yufenglee/subscriptions",
            "organizations_url": "https://api.github.com/users/yufenglee/orgs",
            "repos_url": "https://api.github.com/users/yufenglee/repos",
            "events_url": "https://api.github.com/users/yufenglee/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yufenglee/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-03-30T02:04:57Z",
        "updated_at": "2021-03-30T02:04:57Z",
        "author_association": "MEMBER",
        "body": "The quantization tool does provide QAT ability. It depends on the original framework to do it. Once it is re-trained and converted to ONNX, you can call this quantize_qat API to quantize the model to int8. ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/809849005/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/810096447",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/7144#issuecomment-810096447",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/7144",
        "id": 810096447,
        "node_id": "MDEyOklzc3VlQ29tbWVudDgxMDA5NjQ0Nw==",
        "user": {
            "login": "Kentaro-Mikami",
            "id": 81408025,
            "node_id": "MDQ6VXNlcjgxNDA4MDI1",
            "avatar_url": "https://avatars.githubusercontent.com/u/81408025?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Kentaro-Mikami",
            "html_url": "https://github.com/Kentaro-Mikami",
            "followers_url": "https://api.github.com/users/Kentaro-Mikami/followers",
            "following_url": "https://api.github.com/users/Kentaro-Mikami/following{/other_user}",
            "gists_url": "https://api.github.com/users/Kentaro-Mikami/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Kentaro-Mikami/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Kentaro-Mikami/subscriptions",
            "organizations_url": "https://api.github.com/users/Kentaro-Mikami/orgs",
            "repos_url": "https://api.github.com/users/Kentaro-Mikami/repos",
            "events_url": "https://api.github.com/users/Kentaro-Mikami/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Kentaro-Mikami/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-03-30T10:10:33Z",
        "updated_at": "2021-03-30T10:10:33Z",
        "author_association": "NONE",
        "body": "Hello, Yufeng. Thank you for your reply.\r\nBut I'm a little confusing, would you tell me more?\r\nWhat is the difference between \"quantize_dynamic\" and \"quantize_qat\"?\r\nIf we use \"quantize_dynamic\", we use original framework for training and creating the model. After converting the model to onnx, we use \"quantize_dynamic\" for converting to int8. I think that this procedure is same as your explanation.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/810096447/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/810397282",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/7144#issuecomment-810397282",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/7144",
        "id": 810397282,
        "node_id": "MDEyOklzc3VlQ29tbWVudDgxMDM5NzI4Mg==",
        "user": {
            "login": "yufenglee",
            "id": 30486710,
            "node_id": "MDQ6VXNlcjMwNDg2NzEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/30486710?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yufenglee",
            "html_url": "https://github.com/yufenglee",
            "followers_url": "https://api.github.com/users/yufenglee/followers",
            "following_url": "https://api.github.com/users/yufenglee/following{/other_user}",
            "gists_url": "https://api.github.com/users/yufenglee/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yufenglee/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yufenglee/subscriptions",
            "organizations_url": "https://api.github.com/users/yufenglee/orgs",
            "repos_url": "https://api.github.com/users/yufenglee/repos",
            "events_url": "https://api.github.com/users/yufenglee/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yufenglee/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-03-30T16:20:05Z",
        "updated_at": "2021-03-30T16:20:05Z",
        "author_association": "MEMBER",
        "body": "If you do QAT retraining with either tf/pytorch, those frameworks add fakequant ops to simulate the quantization process. When you convert/export a QAT model to onnx, onnx uses QuantizeLinear/DeQuantizeLinear pair to represent the fakequant ops. If you call the quantize_dynamic directly, those D/DQs will be kept in the quantized model. The quantize_qat extracts the quantization parameter from the Q/DQ and throw them. That's the difference.\r\n\r\nWe are deprecating the method quantize_qat. [ORT ](https://github.com/microsoft/onnxruntime/pull/7033)is supporting to run model with Q/DQs directly. For QAT models, you don't need to go through the quantization tool anymore once the work is done. Now our latest master already has basic support. You can try it on your QAT model.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/810397282/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/940019612",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/7144#issuecomment-940019612",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/7144",
        "id": 940019612,
        "node_id": "IC_kwDOCVq1mM44B4-c",
        "user": {
            "login": "BeginnerW",
            "id": 28683497,
            "node_id": "MDQ6VXNlcjI4NjgzNDk3",
            "avatar_url": "https://avatars.githubusercontent.com/u/28683497?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/BeginnerW",
            "html_url": "https://github.com/BeginnerW",
            "followers_url": "https://api.github.com/users/BeginnerW/followers",
            "following_url": "https://api.github.com/users/BeginnerW/following{/other_user}",
            "gists_url": "https://api.github.com/users/BeginnerW/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/BeginnerW/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/BeginnerW/subscriptions",
            "organizations_url": "https://api.github.com/users/BeginnerW/orgs",
            "repos_url": "https://api.github.com/users/BeginnerW/repos",
            "events_url": "https://api.github.com/users/BeginnerW/events{/privacy}",
            "received_events_url": "https://api.github.com/users/BeginnerW/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-10-11T13:13:30Z",
        "updated_at": "2021-10-11T13:13:30Z",
        "author_association": "NONE",
        "body": "> If you do QAT retraining with either tf/pytorch, those frameworks add fakequant ops to simulate the quantization process. When you convert/export a QAT model to onnx, onnx uses QuantizeLinear/DeQuantizeLinear pair to represent the fakequant ops. If you call the quantize_dynamic directly, those D/DQs will be kept in the quantized model. The quantize_qat extracts the quantization parameter from the Q/DQ and throw them. That's the difference.\r\n> \r\n> We are deprecating the method quantize_qat. [ORT ](https://github.com/microsoft/onnxruntime/pull/7033)is supporting to run model with Q/DQs directly. For QAT models, you don't need to go through the quantization tool anymore once the work is done. Now our latest master already has basic support. You can try it on your QAT model.\r\n\r\nfrom what i know, pytorch does not support export a QAT model to onnx。would you give some advice on pytorch QAT model exporting",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/940019612/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1012644900",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/7144#issuecomment-1012644900",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/7144",
        "id": 1012644900,
        "node_id": "IC_kwDOCVq1mM48W7wk",
        "user": {
            "login": "Hrayo712",
            "id": 34102313,
            "node_id": "MDQ6VXNlcjM0MTAyMzEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/34102313?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Hrayo712",
            "html_url": "https://github.com/Hrayo712",
            "followers_url": "https://api.github.com/users/Hrayo712/followers",
            "following_url": "https://api.github.com/users/Hrayo712/following{/other_user}",
            "gists_url": "https://api.github.com/users/Hrayo712/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Hrayo712/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Hrayo712/subscriptions",
            "organizations_url": "https://api.github.com/users/Hrayo712/orgs",
            "repos_url": "https://api.github.com/users/Hrayo712/repos",
            "events_url": "https://api.github.com/users/Hrayo712/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Hrayo712/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-14T00:37:03Z",
        "updated_at": "2022-01-14T00:37:03Z",
        "author_association": "NONE",
        "body": "@yufenglee, when you say the ORT master branch has basic support for running Q/DQ models directly, what do you mean exactly. What are the current limitations for running Q/DQ models directly with ORT ? is this only limited to the CPU EP ? or\r\n is running Q/DQ models supported by other EPs, such as NNAPI for example ?\r\n\r\nThanks in advance for your support",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1012644900/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1015608497",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/7144#issuecomment-1015608497",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/7144",
        "id": 1015608497,
        "node_id": "IC_kwDOCVq1mM48iPSx",
        "user": {
            "login": "yufenglee",
            "id": 30486710,
            "node_id": "MDQ6VXNlcjMwNDg2NzEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/30486710?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yufenglee",
            "html_url": "https://github.com/yufenglee",
            "followers_url": "https://api.github.com/users/yufenglee/followers",
            "following_url": "https://api.github.com/users/yufenglee/following{/other_user}",
            "gists_url": "https://api.github.com/users/yufenglee/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yufenglee/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yufenglee/subscriptions",
            "organizations_url": "https://api.github.com/users/yufenglee/orgs",
            "repos_url": "https://api.github.com/users/yufenglee/repos",
            "events_url": "https://api.github.com/users/yufenglee/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yufenglee/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-18T16:49:23Z",
        "updated_at": "2022-01-18T16:49:23Z",
        "author_association": "MEMBER",
        "body": "ORT can run Q/DQ models now. Currently, CPU, TensorRT, NNAPI EP have optimization for it. OpenVino is adding support. On other EPs, you may see perf issue. And tensorRT EP needs both activation and weight are quantized symmetrically. ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1015608497/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1024307369",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/7144#issuecomment-1024307369",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/7144",
        "id": 1024307369,
        "node_id": "IC_kwDOCVq1mM49DbCp",
        "user": {
            "login": "Hrayo712",
            "id": 34102313,
            "node_id": "MDQ6VXNlcjM0MTAyMzEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/34102313?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Hrayo712",
            "html_url": "https://github.com/Hrayo712",
            "followers_url": "https://api.github.com/users/Hrayo712/followers",
            "following_url": "https://api.github.com/users/Hrayo712/following{/other_user}",
            "gists_url": "https://api.github.com/users/Hrayo712/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Hrayo712/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Hrayo712/subscriptions",
            "organizations_url": "https://api.github.com/users/Hrayo712/orgs",
            "repos_url": "https://api.github.com/users/Hrayo712/repos",
            "events_url": "https://api.github.com/users/Hrayo712/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Hrayo712/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-28T15:04:44Z",
        "updated_at": "2022-01-28T15:06:07Z",
        "author_association": "NONE",
        "body": "@yufenglee thanks for your reply.\r\n\r\nDue to project needs, I am interested in the functionality of `quantize_qat()`, that is, to convert a QAT model (exported to ONNX from Pytorch), to its Quantized ops representation (QLinearConv, QLinearMatMul, etc.).\r\n\r\nI have the following toy QAT model (borrowed from Pytorch QAT example):\r\n```\r\nclass M(torch.nn.Module):\r\n    def __init__(self):\r\n        super(M, self).__init__()\r\n        # QuantStub converts tensors from floating point to quantized\r\n        self.quant = torch.quantization.QuantStub()\r\n        self.conv = torch.nn.Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\r\n        self.bn = torch.nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n        self.relu = torch.nn.ReLU()\r\n        # DeQuantStub converts tensors from quantized to floating point\r\n        self.dequant = torch.quantization.DeQuantStub()\r\n\r\n    def forward(self, x):\r\n        x = self.quant(x)\r\n        x = self.conv(x)\r\n        x = self.bn(x)\r\n        x = self.relu(x)\r\n        x = self.dequant(x)\r\n        return x\r\n```\r\n\r\nThen, I export this model to ONNX as follows:\r\n```\r\nqat_model= M().eval()\r\n\r\nqat_model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\r\n\r\nqat_model= torch.quantization.fuse_modules(qat_model, [[\"conv\", \"bn\", \"relu\"]])\r\n\r\nquantization.prepare_qat(qat_model, inplace=True)\r\n\r\ndummy_input = torch.randn(1, 3, 224, 224)\r\n\r\nqat_model.apply(quantization.disable_observer) # observers must be disabled before export\r\n\r\ntorch.onnx.export(qat_model,\r\n                  dummy_input,\r\n                  \"model.onnx\",\r\n                  opset_version=13)\r\n```\r\n\r\nInspecting the ONNX model via Netron shows me that the export functionality is correctly converting the QAT model to ONNX via introducing QuantizeLinear/DequantizeLinear (Q/DQ) pairs. Finally, I use the `quantize_qat()` interface as follows:\r\n\r\n```\r\nquantize_qat(\"dummy_model.onnx\", \"dummy_qdq_to_qop.onnx\")\r\n```\r\n\r\nHowever, this I am getting the following error from `onnx_quantizer.py`:\r\n\r\n`ValueError: Remove fake-quantized node pair Error: Parent node is not found for QuantizeLinear_2.`\r\n\r\nAny insight into what might be the problem ?\r\n\r\n**System information**\r\n> Torch 1.9.0\r\n> onnxruntime 1.8.2\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1024307369/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1024437474",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/7144#issuecomment-1024437474",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/7144",
        "id": 1024437474,
        "node_id": "IC_kwDOCVq1mM49D6zi",
        "user": {
            "login": "yufenglee",
            "id": 30486710,
            "node_id": "MDQ6VXNlcjMwNDg2NzEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/30486710?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yufenglee",
            "html_url": "https://github.com/yufenglee",
            "followers_url": "https://api.github.com/users/yufenglee/followers",
            "following_url": "https://api.github.com/users/yufenglee/following{/other_user}",
            "gists_url": "https://api.github.com/users/yufenglee/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yufenglee/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yufenglee/subscriptions",
            "organizations_url": "https://api.github.com/users/yufenglee/orgs",
            "repos_url": "https://api.github.com/users/yufenglee/repos",
            "events_url": "https://api.github.com/users/yufenglee/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yufenglee/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-28T17:19:34Z",
        "updated_at": "2022-01-28T17:20:19Z",
        "author_association": "MEMBER",
        "body": "@Hrayo712, OnnxRuntime supports to dump out optimized model with SessionOptions::optimized_model_filepath. Could you please try code like below?\r\n\r\n```\r\ndef generate_identified_filename(filename: Path, identifier: str) -> Path:\r\n    '''\r\n    Helper function to generate a identifiable filepath by concatenating the given identifier as a suffix.   \r\n    '''\r\n    return filename.parent.joinpath(filename.stem + identifier).with_suffix(filename.suffix)\r\n\r\ndef dump_optimize_model(model_path: Path):\r\n    '''\r\n        Generate model that applies graph optimization (constant folding,etc.)\r\n        parameter model_path: path to the original onnx model\r\n        return: optimized onnx model\r\n    '''\r\n    opt_model_path = generate_identified_filename(model_path, \"-opt\")\r\n    sess_option = SessionOptions()\r\n    sess_option.optimized_model_filepath = opt_model_path.as_posix()\r\n    sess_option.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_EXTENDED\r\n    _ = InferenceSession(model_path.as_posix(), sess_option, providers=['CPUExecutionProvider'])\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1024437474/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1024449481",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/7144#issuecomment-1024449481",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/7144",
        "id": 1024449481,
        "node_id": "IC_kwDOCVq1mM49D9vJ",
        "user": {
            "login": "Hrayo712",
            "id": 34102313,
            "node_id": "MDQ6VXNlcjM0MTAyMzEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/34102313?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Hrayo712",
            "html_url": "https://github.com/Hrayo712",
            "followers_url": "https://api.github.com/users/Hrayo712/followers",
            "following_url": "https://api.github.com/users/Hrayo712/following{/other_user}",
            "gists_url": "https://api.github.com/users/Hrayo712/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Hrayo712/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Hrayo712/subscriptions",
            "organizations_url": "https://api.github.com/users/Hrayo712/orgs",
            "repos_url": "https://api.github.com/users/Hrayo712/repos",
            "events_url": "https://api.github.com/users/Hrayo712/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Hrayo712/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-28T17:34:30Z",
        "updated_at": "2022-01-28T17:35:11Z",
        "author_association": "NONE",
        "body": "@yufenglee , you mean optimizing the graph with this method before feeding it to `quantize_qat()` ? I tried and it didnt work. \r\n\r\nAs far as I understand, what you mention will only perform graph optimizations. What I need is to go from a Q/DQ model (which I obtained by exportin from Pytorch to ONNX) to Quantized ops. That is the purpose of the `quantize_qat()` interface, right ? ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1024449481/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]