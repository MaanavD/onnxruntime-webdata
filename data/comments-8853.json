[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/907554808",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8853#issuecomment-907554808",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8853",
        "id": 907554808,
        "node_id": "IC_kwDOCVq1mM42GC_4",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-08-28T02:41:20Z",
        "updated_at": "2021-08-28T02:41:20Z",
        "author_association": "MEMBER",
        "body": "@jplu, compare_bert_results tool assumes that two ONNX models has same input data type. If --input_int32 is used, input_ids in original onnx model is int64, and optimized model has int32 inputs.\r\n\r\nAs walkaround, you can generate a model without --input_int32 for parity verification. Once the verification is done, you can add --input_int32 to generate the final model for performance test etc.\r\n\r\nHere is example commands to run from script of master branch:\r\n```\r\ngit clone https://github.com/microsoft/onnxruntime\r\ncd onnxruntime/onnxruntime/python/tools/transformers\r\npython -m transformers.onnx --model=bert-base-cased onnx/bert-base-cased/\r\npython optimizer.py --input onnx/bert-base-cased/model.onnx --output onnx/bert-base-cased/model_opt.onnx --model_type bert --float16\r\npython compare_bert_results.py --baseline_model onnx/bert-base-cased/model.onnx --optimized_model onnx/bert-base-cased/model_opt.onnx --input_ids input_ids --segment_ids token_type_ids --input_mask attention_mask --batch_size 1 --sequence_length 128 --samples 100\r\npython optimizer.py --input onnx/bert-base-cased/model.onnx --output onnx/bert-base-cased/model_int32.onnx --model_type bert --float16 --input_int32\r\npython bert_perf_test.py --model onnx/bert-base-cased/model_int32.onnx --use_gpu -b 1 -s 128 -t 100 --opt_level 99\r\n```\r\n\r\nExample outputs:\r\n```\r\n>python optimizer.py --input onnx/bert-base-cased/model.onnx --output onnx/bert-base-cased/model_opt.onnx --model_type bert --float16\r\n               apply: Fused LayerNormalization count: 25\r\n               apply: Fused Gelu count: 12\r\n               apply: Fused SkipLayerNormalization count: 24\r\n               apply: Fused Attention count: 12\r\n         prune_graph: Graph pruned: 0 inputs, 0 outputs and 5 nodes are removed\r\n               apply: Fused EmbedLayerNormalization(with mask) count: 1\r\n         prune_graph: Graph pruned: 0 inputs, 0 outputs and 4 nodes are removed\r\n         prune_graph: Graph pruned: 0 inputs, 0 outputs and 0 nodes are removed\r\n               apply: Fused BiasGelu count: 12\r\n               apply: Fused SkipLayerNormalization(add bias) count: 24\r\n            optimize: opset verion: 12\r\n  save_model_to_file: Sort graphs in topological order\r\n  save_model_to_file: Output model to onnx/bert-base-cased/model_opt.onnx\r\nget_fused_operator_statistics: Optimized operators:{'EmbedLayerNormalization': 1, 'Attention': 12, 'Gelu': 0, 'FastGelu': 0, 'BiasGelu': 12, 'LayerNormalization': 0, 'SkipLayerNormalization': 24}\r\n                main: The model has been fully optimized.\r\n\r\n>python compare_bert_results.py --baseline_model onnx/bert-base-cased/model.onnx --optimized_model onnx/bert-base-cased/model_opt.onnx --input_ids input_ids --segment_ids token_type_ids --input_mask attention_mask --batch_size 1 --sequence_length 128 --samples 100\r\nWARNING: 100 out of 100 results NOT passed for thresholds (rtol=0.001, atol=0.0001).\r\nmaximum absolute difference=0.008105754852294922\r\nmaximum relative difference=1947.8863525390625\r\n```\r\nThe threshold is for fp32 model. For fp16 model, I think the maximum absolute difference is expected (since the resolution of fp16 is around 0.001).",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/907554808/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/907594690",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8853#issuecomment-907594690",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8853",
        "id": 907594690,
        "node_id": "IC_kwDOCVq1mM42GMvC",
        "user": {
            "login": "jplu",
            "id": 959590,
            "node_id": "MDQ6VXNlcjk1OTU5MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/959590?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jplu",
            "html_url": "https://github.com/jplu",
            "followers_url": "https://api.github.com/users/jplu/followers",
            "following_url": "https://api.github.com/users/jplu/following{/other_user}",
            "gists_url": "https://api.github.com/users/jplu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jplu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jplu/subscriptions",
            "organizations_url": "https://api.github.com/users/jplu/orgs",
            "repos_url": "https://api.github.com/users/jplu/repos",
            "events_url": "https://api.github.com/users/jplu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jplu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-08-28T08:40:15Z",
        "updated_at": "2021-08-28T08:40:15Z",
        "author_association": "NONE",
        "body": "Thanks @tianleiwu I tried what you suggested and I get the same error with the command:\r\n```\r\npython -m onnxruntime.transformers.bert_perf_test --model  onnx/bert-base-cased/model_opt.onnx --opt_level 99 -b 1 -s 128 -t 100 --input_ids_name input_ids --input_mask_name attention_mask --segment_ids token_type_ids --use_gpu\r\n```\r\n\r\n```\r\ntest setting TestSetting(batch_size=1, sequence_length=128, test_cases=10, test_times=100, use_gpu=True, intra_op_num_threads=None, seed=3, verbose=False)\r\nGenerating 10 samples for batch_size=1 sequence_length=128\r\nProcess Process-2:\r\nTraceback (most recent call last):\r\n  File \"/home/jplu/anaconda3/envs/indexing/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\r\n    self.run()\r\n  File \"/home/jplu/anaconda3/envs/indexing/lib/python3.8/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/jplu/anaconda3/envs/indexing/lib/python3.8/site-packages/onnxruntime/transformers/bert_perf_test.py\", line 116, in run_one_test\r\n    session = create_session(model_setting.model_path, test_setting.use_gpu, intra_op_num_threads,\r\n  File \"/home/jplu/anaconda3/envs/indexing/lib/python3.8/site-packages/onnxruntime/transformers/bert_perf_test.py\", line 84, in create_session\r\n    session = onnxruntime.InferenceSession(model_path, sess_options, providers=execution_providers)\r\n  File \"/home/jplu/anaconda3/envs/indexing/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\", line 283, in __init__\r\n    self._create_inference_session(providers, provider_options, disabled_optimizers)\r\n  File \"/home/jplu/anaconda3/envs/indexing/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\", line 310, in _create_inference_session\r\n    sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)\r\nonnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : Load model from onnx/encoder/model_opt.onnx failed:/onnxruntime_src/onnxruntime/core/graph/model.cc:111 onnxruntime::Model::Model(onnx::ModelProto&&, const PathString&, const IOnnxRuntimeOpSchemaRegistryList*, const onnxruntime::logging::Logger&) Unknown model file format version.\r\n```\r\n\r\nThe exact same error happen also when I try to just open the model:\r\n```\r\nimport onnxruntime as ort\r\n\r\nort_session = ort.InferenceSession(\"onnx/bert-base-cased/model_opt.onnx\")\r\n```\r\n\r\nWhat you suggest apparently doesn't seems to work :(  any other walkaround, or is that a real bug?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/907594690/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/907645061",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8853#issuecomment-907645061",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8853",
        "id": 907645061,
        "node_id": "IC_kwDOCVq1mM42GZCF",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-08-28T15:50:37Z",
        "updated_at": "2021-08-28T16:13:05Z",
        "author_association": "MEMBER",
        "body": "@jplu, It is working in my machine.\r\n\r\n`Unknown model file format version` is probably caused by related packages (like protobuf). You can check related packages like the following Python script:\r\n```\r\nimport pkg_resources\r\ninstalled_packages = pkg_resources.working_set\r\nrelated_packages = ['flatbuffers', 'numpy', 'onnx', 'onnxconverter-common', 'onnxruntime-gpu', 'onnxruntime', 'ort-nightly-gpu', 'ort-nightly', 'protobuf', 'sympy', 'torch', 'transformers']\r\nrelated_packages_list = sorted([\"%s==%s\" % (i.key, i.version) for i in installed_packages if i.key in related_packages])\r\nprint(related_packages_list)\r\n```\r\nIn my machine, the output is like\r\n```\r\n['flatbuffers==2.0', 'numpy==1.19.5', 'onnx==1.9.0', 'onnxconverter-common==1.9.0', 'onnxruntime-gpu==1.8.1', 'protobuf==3.17.3', 'sympy==1.5', 'torch==1.9.0+cu111', 'transformers==4.9.2']\r\n```\r\nAlso 1.8.1 need CUDA 11.0. Not sure it could work with CUDA 11.2.\r\n\r\nBTW, you will need run optimizer.py in master branch (instead of 1.8.1) to get fully optimized model, since 1.8.1 does not \r\n support the latest transformers. Also, remove --opt_level 99 in using optimizer.py, otherwise the model cannot be fully optimized.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/907645061/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/907648394",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8853#issuecomment-907648394",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8853",
        "id": 907648394,
        "node_id": "IC_kwDOCVq1mM42GZ2K",
        "user": {
            "login": "jplu",
            "id": 959590,
            "node_id": "MDQ6VXNlcjk1OTU5MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/959590?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jplu",
            "html_url": "https://github.com/jplu",
            "followers_url": "https://api.github.com/users/jplu/followers",
            "following_url": "https://api.github.com/users/jplu/following{/other_user}",
            "gists_url": "https://api.github.com/users/jplu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jplu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jplu/subscriptions",
            "organizations_url": "https://api.github.com/users/jplu/orgs",
            "repos_url": "https://api.github.com/users/jplu/repos",
            "events_url": "https://api.github.com/users/jplu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jplu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-08-28T16:15:02Z",
        "updated_at": "2021-08-28T16:15:02Z",
        "author_association": "NONE",
        "body": "Here is my output:\r\n```\r\n['flatbuffers==2.0', 'numpy==1.20.3', 'onnx==1.10.1', 'onnxconverter-common==1.8.1', 'onnxruntime-gpu==1.8.1', 'protobuf==3.17.3', 'sympy==1.8', 'torch==1.9.0+cu111', 'transformers==4.9.2']\r\n```\r\n\r\n> BTW, you will need run optimizer.py in master branch (instead of 1.8.1) to get fully optimized model, since 1.8.1 does not\r\nsupport the latest transformers.\r\n\r\nI can use only pypi packages for now because of some reproducible reasons.\r\n\r\n> Also, remove --opt_level 99 in using optimizer.py, otherwise the model cannot be fully optimized.\r\n\r\nThanks for the hint! Good to know! Might be worth to specify this in the doc. And, by curiosity, why with this option the model cannot be fully optimized?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/907648394/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/907727044",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8853#issuecomment-907727044",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8853",
        "id": 907727044,
        "node_id": "IC_kwDOCVq1mM42GtDE",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-08-29T04:24:32Z",
        "updated_at": "2021-08-29T04:26:04Z",
        "author_association": "MEMBER",
        "body": "@jplu, I reproduced the problem with onnx 1.10.1. You can downgrade onnx to 1.9.0 to walkaround the issue. When 1.8.1 was released, onnx was at 1.9.0. I will investigate the root cause later.\r\n\r\n--opt_level 99 will use onnxruntime to do graph optimization (written in C++). It has some extra optimizations that could change the graph, which will cause no match in attention fusion in python later. ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/907727044/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/908147273",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8853#issuecomment-908147273",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8853",
        "id": 908147273,
        "node_id": "IC_kwDOCVq1mM42ITpJ",
        "user": {
            "login": "jplu",
            "id": 959590,
            "node_id": "MDQ6VXNlcjk1OTU5MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/959590?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jplu",
            "html_url": "https://github.com/jplu",
            "followers_url": "https://api.github.com/users/jplu/followers",
            "following_url": "https://api.github.com/users/jplu/following{/other_user}",
            "gists_url": "https://api.github.com/users/jplu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jplu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jplu/subscriptions",
            "organizations_url": "https://api.github.com/users/jplu/orgs",
            "repos_url": "https://api.github.com/users/jplu/repos",
            "events_url": "https://api.github.com/users/jplu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jplu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-08-30T08:23:27Z",
        "updated_at": "2021-08-30T08:23:27Z",
        "author_association": "NONE",
        "body": ">@jplu, I reproduced the problem with onnx 1.10.1. You can downgrade onnx to 1.9.0 to walkaround the issue. When 1.8.1 was released, onnx was at 1.9.0. I will investigate the root cause later.\r\n\r\nPerfect, thanks a lot @tianleiwu for the walkaround. Happy to have helped to find a potential bug :)\r\n\r\n>--opt_level 99 will use onnxruntime to do graph optimization (written in C++). It has some extra optimizations that could change the graph, which will cause no match in attention fusion in python later.\r\n\r\nHumm I see. Do you suggest to use only `--opt_level` set to 1 or 2? Or no `--opt_level` at all?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/908147273/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/908176732",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8853#issuecomment-908176732",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8853",
        "id": 908176732,
        "node_id": "IC_kwDOCVq1mM42Ia1c",
        "user": {
            "login": "jplu",
            "id": 959590,
            "node_id": "MDQ6VXNlcjk1OTU5MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/959590?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jplu",
            "html_url": "https://github.com/jplu",
            "followers_url": "https://api.github.com/users/jplu/followers",
            "following_url": "https://api.github.com/users/jplu/following{/other_user}",
            "gists_url": "https://api.github.com/users/jplu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jplu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jplu/subscriptions",
            "organizations_url": "https://api.github.com/users/jplu/orgs",
            "repos_url": "https://api.github.com/users/jplu/repos",
            "events_url": "https://api.github.com/users/jplu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jplu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-08-30T09:08:53Z",
        "updated_at": "2021-08-30T09:08:53Z",
        "author_association": "NONE",
        "body": "It seems there might be another issue, even though the model can now be properly loaded it raises the following error when the inputs are in `int32`:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jplu/anaconda3/envs/indexing/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\", line 188, in run\r\n    return self._sess.run(output_names, input_feed, run_options)\r\nonnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Unexpected input data type. Actual: (tensor(int32)) , expected: (tensor(int64))\r\n```\r\nThe command used to get the model is still similar to what I proposed:\r\n```\r\npython -m onnxruntime.transformers.optimizer --input onnx/bert-base-cased/model.onnx --output onnx/bert-base-cased/model_opt.onnx --disable_embed_layer_norm --model_type bert --num_heads 12 --hidden_size 768 --float16 --input_int32\r\n```\r\n\r\nHere also the small piece of code I used:\r\n```\r\nimport onnxruntime as ort\r\nfrom transformers import AutoTokenizer\r\n\r\nort_session = ort.InferenceSession(\"onnx/bert-base-cased/model_opt.onnx\")\r\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\r\ninputs = tokenizer(\"Hello there!!\", return_tensors=\"np\")\r\ninputs = {key: val.astype(np.int32) for key, val in inputs.items()}\r\nort_session.run([\"last_hidden_state\", \"pooler_output\"], dict(inputs))\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/908176732/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/908894101",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8853#issuecomment-908894101",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8853",
        "id": 908894101,
        "node_id": "IC_kwDOCVq1mM42LJ-V",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-08-31T04:42:31Z",
        "updated_at": "2021-08-31T04:42:31Z",
        "author_association": "MEMBER",
        "body": "@jplu, the --input_int32 is designed for model with EmbedLayerNormalization. If you disable it (--disable_embed_layer_norm), only attention mask will be converted to int32, and other two inputs are still int64. You can open the onnx file with Netron and inspect the input data types.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/908894101/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/908994706",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8853#issuecomment-908994706",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8853",
        "id": 908994706,
        "node_id": "IC_kwDOCVq1mM42LiiS",
        "user": {
            "login": "jplu",
            "id": 959590,
            "node_id": "MDQ6VXNlcjk1OTU5MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/959590?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jplu",
            "html_url": "https://github.com/jplu",
            "followers_url": "https://api.github.com/users/jplu/followers",
            "following_url": "https://api.github.com/users/jplu/following{/other_user}",
            "gists_url": "https://api.github.com/users/jplu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jplu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jplu/subscriptions",
            "organizations_url": "https://api.github.com/users/jplu/orgs",
            "repos_url": "https://api.github.com/users/jplu/repos",
            "events_url": "https://api.github.com/users/jplu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jplu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-08-31T08:01:06Z",
        "updated_at": "2021-08-31T08:53:01Z",
        "author_association": "NONE",
        "body": "The thing is that even if I remove the `--disable_embed_layer_norm` I get the exact same error. Is that normal?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/908994706/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/909042800",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8853#issuecomment-909042800",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8853",
        "id": 909042800,
        "node_id": "IC_kwDOCVq1mM42LuRw",
        "user": {
            "login": "jplu",
            "id": 959590,
            "node_id": "MDQ6VXNlcjk1OTU5MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/959590?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jplu",
            "html_url": "https://github.com/jplu",
            "followers_url": "https://api.github.com/users/jplu/followers",
            "following_url": "https://api.github.com/users/jplu/following{/other_user}",
            "gists_url": "https://api.github.com/users/jplu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jplu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jplu/subscriptions",
            "organizations_url": "https://api.github.com/users/jplu/orgs",
            "repos_url": "https://api.github.com/users/jplu/repos",
            "events_url": "https://api.github.com/users/jplu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jplu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-08-31T09:01:21Z",
        "updated_at": "2021-08-31T09:01:21Z",
        "author_association": "NONE",
        "body": "Apparently Netron shows that `input_ids` and `token_type_ids` are in int32 but `attention_mask` is still in int64. Here some screenshots\r\n![Capture d’écran de 2021-08-31 10-59-45](https://user-images.githubusercontent.com/959590/131474303-1e145c00-ca22-4806-9e7d-d84b52338c6f.png)\r\n![Capture d’écran de 2021-08-31 11-00-22](https://user-images.githubusercontent.com/959590/131474319-d7f913f6-50f5-409e-957e-ae8257ea4b41.png)\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/909042800/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]